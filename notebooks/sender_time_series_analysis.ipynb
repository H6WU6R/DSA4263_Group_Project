{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sender Time Series Analysis for Fraud Detection\n",
    "\n",
    "This notebook performs comprehensive temporal analysis of sender behavior with focus on:\n",
    "1. **Frequency-based features** at multiple time scales\n",
    "2. **Temporal patterns** that distinguish phishing from legitimate senders\n",
    "3. **Burst detection** and campaign identification\n",
    "4. **Statistical validation** of discriminative features\n",
    "\n",
    "**Goal:** Extract temporal features that complement graph-based features for fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded successfully\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.4\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats, signal\n",
    "from scipy.stats import entropy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, mutual_info_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/cindy/Desktop/DSA4263_Group_Project/data/processed/cleaned_date_merge.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Update this path to your actual data location\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/Users/cindy/Desktop/DSA4263_Group_Project/data/processed/cleaned_date_merge.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m=\u001b[33m'\u001b[39m\u001b[33mISO8601\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Sort by sender and date for time series analysis\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/DSA4263_Group_Project/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/DSA4263_Group_Project/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/DSA4263_Group_Project/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/DSA4263_Group_Project/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/DSA4263_Group_Project/.venv/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/cindy/Desktop/DSA4263_Group_Project/data/processed/cleaned_date_merge.csv'"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Update this path to your actual data location\n",
    "df = pd.read_csv(\"/Users/cindy/Desktop/DSA4263_Group_Project/data/processed/cleaned_date_merge.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'], format='ISO8601')\n",
    "\n",
    "# Sort by sender and date for time series analysis\n",
    "df = df.sort_values(['sender', 'date'])\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"  ‚Ä¢ Total emails: {len(df):,}\")\n",
    "print(f\"  ‚Ä¢ Unique senders: {df['sender'].nunique():,}\")\n",
    "print(f\"  ‚Ä¢ Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"  ‚Ä¢ Phishing emails: {(df['label']==1).sum():,} ({(df['label']==1).mean()*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Legitimate emails: {(df['label']==0).sum():,} ({(df['label']==0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Multi-Scale Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MULTI-SCALE FREQUENCY FEATURE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_frequency_features(sender_df):\n",
    "    \"\"\"\n",
    "    Calculate frequency-based features at multiple time scales with percentiles\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Basic info\n",
    "    features['total_emails'] = len(sender_df)\n",
    "    features['label'] = sender_df['label'].mode()[0] if len(sender_df['label'].mode()) > 0 else sender_df['label'].iloc[0]\n",
    "    \n",
    "    if len(sender_df) == 1:\n",
    "        # Single email sender - set defaults\n",
    "        features['emails_per_minute_max'] = 1\n",
    "        features['emails_per_5min_max'] = 1\n",
    "        features['emails_per_15min_max'] = 1\n",
    "        features['emails_per_30min_max'] = 1  # NEW: 30-minute window\n",
    "        features['emails_per_hour_max'] = 1\n",
    "        features['emails_per_day_max'] = 1\n",
    "        features['burst_ratio'] = 1\n",
    "        features['concentration_1hr'] = 1\n",
    "        features['active_duration_hours'] = 0\n",
    "        features['spikiness_5min'] = 1  # NEW: Spikiness metric\n",
    "        return features\n",
    "    \n",
    "    # Sort by date\n",
    "    sender_df = sender_df.sort_values('date')\n",
    "    \n",
    "    # Calculate time span\n",
    "    time_span = (sender_df['date'].max() - sender_df['date'].min())\n",
    "    features['active_duration_hours'] = time_span.total_seconds() / 3600\n",
    "    \n",
    "    # Create continuous time series with 1-minute bins\n",
    "    start_time = sender_df['date'].min().floor('min')\n",
    "    end_time = sender_df['date'].max().ceil('min')\n",
    "    \n",
    "    # Limit to reasonable range to avoid memory issues\n",
    "    max_minutes = min(int((end_time - start_time).total_seconds() / 60), 10000)\n",
    "    \n",
    "    if max_minutes > 0:\n",
    "        time_index = pd.date_range(start=start_time, end=end_time, freq='min')[:max_minutes]\n",
    "        ts = pd.Series(0, index=time_index)\n",
    "        \n",
    "        # Count emails per minute\n",
    "        for date in sender_df['date']:\n",
    "            minute_key = date.floor('min')\n",
    "            if minute_key in ts.index:\n",
    "                ts[minute_key] += 1\n",
    "        \n",
    "        # Multi-scale frequency features with PERCENTILES\n",
    "        # 1-minute scale\n",
    "        features['emails_per_minute_max'] = ts.max()\n",
    "        features['emails_per_minute_mean'] = ts[ts > 0].mean() if (ts > 0).any() else 0\n",
    "        features['emails_per_minute_p90'] = ts.quantile(0.90)  # NEW\n",
    "        features['emails_per_minute_p95'] = ts.quantile(0.95)  # NEW\n",
    "        features['emails_per_minute_p99'] = ts.quantile(0.99)  # NEW\n",
    "        \n",
    "        # 5-minute scale\n",
    "        if len(ts) >= 5:\n",
    "            ts_5min = ts.rolling(5, min_periods=1).sum()\n",
    "            features['emails_per_5min_max'] = ts_5min.max()\n",
    "            features['emails_per_5min_p90'] = ts_5min.quantile(0.90)  # NEW\n",
    "            features['emails_per_5min_p95'] = ts_5min.quantile(0.95)\n",
    "            features['emails_per_5min_p99'] = ts_5min.quantile(0.99)  # NEW\n",
    "            features['emails_per_5min_median'] = ts_5min.median()  # NEW\n",
    "            # Spikiness: max/median ratio\n",
    "            features['spikiness_5min'] = ts_5min.max() / ts_5min.median() if ts_5min.median() > 0 else 1  # NEW\n",
    "        else:\n",
    "            features['emails_per_5min_max'] = features['total_emails']\n",
    "            features['emails_per_5min_p90'] = features['total_emails']\n",
    "            features['emails_per_5min_p95'] = features['total_emails']\n",
    "            features['emails_per_5min_p99'] = features['total_emails']\n",
    "            features['emails_per_5min_median'] = features['total_emails']\n",
    "            features['spikiness_5min'] = 1\n",
    "        \n",
    "        # 15-minute scale\n",
    "        if len(ts) >= 15:\n",
    "            ts_15min = ts.rolling(15, min_periods=1).sum()\n",
    "            features['emails_per_15min_max'] = ts_15min.max()\n",
    "            features['emails_per_15min_p90'] = ts_15min.quantile(0.90)  # NEW\n",
    "            features['emails_per_15min_p95'] = ts_15min.quantile(0.95)\n",
    "            features['emails_per_15min_p99'] = ts_15min.quantile(0.99)  # NEW\n",
    "            features['spikiness_15min'] = ts_15min.max() / ts_15min.median() if ts_15min.median() > 0 else 1  # NEW\n",
    "        else:\n",
    "            features['emails_per_15min_max'] = features['total_emails']\n",
    "            features['emails_per_15min_p90'] = features['total_emails']\n",
    "            features['emails_per_15min_p95'] = features['total_emails']\n",
    "            features['emails_per_15min_p99'] = features['total_emails']\n",
    "            features['spikiness_15min'] = 1\n",
    "        \n",
    "        # 30-minute scale (NEW - suggested for campaigns)\n",
    "        if len(ts) >= 30:\n",
    "            ts_30min = ts.rolling(30, min_periods=1).sum()\n",
    "            features['emails_per_30min_max'] = ts_30min.max()\n",
    "            features['emails_per_30min_p95'] = ts_30min.quantile(0.95)\n",
    "            features['emails_per_30min_variance'] = ts_30min.var()  # NEW: variance within windows\n",
    "        else:\n",
    "            features['emails_per_30min_max'] = features['total_emails']\n",
    "            features['emails_per_30min_p95'] = features['total_emails']\n",
    "            features['emails_per_30min_variance'] = 0\n",
    "        \n",
    "        # 1-hour scale\n",
    "        if len(ts) >= 60:\n",
    "            ts_hour = ts.rolling(60, min_periods=1).sum()\n",
    "            features['emails_per_hour_max'] = ts_hour.max()\n",
    "            features['emails_per_hour_mean'] = ts_hour[ts_hour > 0].mean() if (ts_hour > 0).any() else 0\n",
    "            features['emails_per_hour_p90'] = ts_hour.quantile(0.90)  # NEW\n",
    "            features['emails_per_hour_p95'] = ts_hour.quantile(0.95)  # NEW\n",
    "            features['concentration_1hr'] = ts_hour.max() / features['total_emails']\n",
    "        else:\n",
    "            features['emails_per_hour_max'] = features['total_emails']\n",
    "            features['emails_per_hour_mean'] = features['total_emails']\n",
    "            features['emails_per_hour_p90'] = features['total_emails']\n",
    "            features['emails_per_hour_p95'] = features['total_emails']\n",
    "            features['concentration_1hr'] = 1.0\n",
    "    else:\n",
    "        # All emails in same minute\n",
    "        features['emails_per_minute_max'] = features['total_emails']\n",
    "        features['emails_per_minute_mean'] = features['total_emails']\n",
    "        features['emails_per_minute_p90'] = features['total_emails']\n",
    "        features['emails_per_minute_p95'] = features['total_emails']\n",
    "        features['emails_per_minute_p99'] = features['total_emails']\n",
    "        features['emails_per_5min_max'] = features['total_emails']\n",
    "        features['emails_per_5min_p90'] = features['total_emails']\n",
    "        features['emails_per_5min_p95'] = features['total_emails']\n",
    "        features['emails_per_5min_p99'] = features['total_emails']\n",
    "        features['emails_per_5min_median'] = features['total_emails']\n",
    "        features['spikiness_5min'] = 1\n",
    "        features['emails_per_15min_max'] = features['total_emails']\n",
    "        features['emails_per_15min_p90'] = features['total_emails']\n",
    "        features['emails_per_15min_p95'] = features['total_emails']\n",
    "        features['emails_per_15min_p99'] = features['total_emails']\n",
    "        features['spikiness_15min'] = 1\n",
    "        features['emails_per_30min_max'] = features['total_emails']\n",
    "        features['emails_per_30min_p95'] = features['total_emails']\n",
    "        features['emails_per_30min_variance'] = 0\n",
    "        features['emails_per_hour_max'] = features['total_emails']\n",
    "        features['emails_per_hour_mean'] = features['total_emails']\n",
    "        features['emails_per_hour_p90'] = features['total_emails']\n",
    "        features['emails_per_hour_p95'] = features['total_emails']\n",
    "        features['concentration_1hr'] = 1.0\n",
    "    \n",
    "    # Daily scale\n",
    "    daily_counts = sender_df.groupby(sender_df['date'].dt.date).size()\n",
    "    features['emails_per_day_max'] = daily_counts.max()\n",
    "    features['emails_per_day_mean'] = daily_counts.mean()\n",
    "    features['active_days'] = len(daily_counts)\n",
    "    \n",
    "    # Burst ratio\n",
    "    if features['emails_per_hour_mean'] > 0:\n",
    "        features['burst_ratio'] = features['emails_per_hour_max'] / features['emails_per_hour_mean']\n",
    "    else:\n",
    "        features['burst_ratio'] = 1\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply to all senders\n",
    "print(\"\\n‚è≥ Extracting frequency features for all senders...\")\n",
    "frequency_features = []\n",
    "for sender, group in df.groupby('sender'):\n",
    "    features = calculate_frequency_features(group)\n",
    "    features['sender'] = sender\n",
    "    frequency_features.append(features)\n",
    "\n",
    "freq_df = pd.DataFrame(frequency_features)\n",
    "print(f\"‚úÖ Frequency features extracted for {len(freq_df)} senders\")\n",
    "print(f\"\\nüìä Feature columns created ({len([c for c in freq_df.columns if c not in ['sender', 'label']])} features):\")\n",
    "feature_cols_freq = [c for c in freq_df.columns if c not in ['sender', 'label']]\n",
    "for i in range(0, len(feature_cols_freq), 4):\n",
    "    print(\"  \", \", \".join(feature_cols_freq[i:i+4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Advanced Temporal Pattern Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ADVANCED TEMPORAL PATTERN EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_temporal_patterns(sender_df):\n",
    "    \"\"\"\n",
    "    Extract advanced temporal patterns and rhythms\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    if len(sender_df) == 1:\n",
    "        # Defaults for single email senders\n",
    "        features['inter_email_mean'] = 0\n",
    "        features['inter_email_std'] = 0\n",
    "        features['inter_email_cv'] = 0\n",
    "        features['hour_entropy'] = 0\n",
    "        features['day_entropy'] = 0\n",
    "        features['time_consistency_score'] = 1\n",
    "        features['is_burst_sender'] = 0\n",
    "        features['night_ratio'] = 1 if sender_df.iloc[0]['date'].hour >= 22 or sender_df.iloc[0]['date'].hour < 6 else 0\n",
    "        features['business_hours_ratio'] = 1 if 9 <= sender_df.iloc[0]['date'].hour < 18 else 0\n",
    "        features['weekend_ratio'] = 1 if sender_df.iloc[0]['date'].dayofweek >= 5 else 0\n",
    "        return features\n",
    "    \n",
    "    # Sort by date\n",
    "    sender_df = sender_df.sort_values('date')\n",
    "    \n",
    "    # Inter-email intervals\n",
    "    intervals = sender_df['date'].diff().dropna().dt.total_seconds() / 60  # in minutes\n",
    "    if len(intervals) > 0:\n",
    "        features['inter_email_mean'] = intervals.mean()\n",
    "        features['inter_email_std'] = intervals.std()\n",
    "        features['inter_email_min'] = intervals.min()\n",
    "        features['inter_email_max'] = intervals.max()\n",
    "        features['inter_email_cv'] = intervals.std() / intervals.mean() if intervals.mean() > 0 else 0\n",
    "        \n",
    "        # Detect burst pattern (many emails with < 5 min intervals)\n",
    "        rapid_emails = (intervals < 5).sum()\n",
    "        features['rapid_succession_ratio'] = rapid_emails / len(intervals)\n",
    "        features['is_burst_sender'] = 1 if features['rapid_succession_ratio'] > 0.5 else 0\n",
    "    else:\n",
    "        features['inter_email_mean'] = 0\n",
    "        features['inter_email_std'] = 0\n",
    "        features['inter_email_min'] = 0\n",
    "        features['inter_email_max'] = 0\n",
    "        features['inter_email_cv'] = 0\n",
    "        features['rapid_succession_ratio'] = 0\n",
    "        features['is_burst_sender'] = 0\n",
    "    \n",
    "    # Temporal entropy (randomness of patterns)\n",
    "    hours = sender_df['date'].dt.hour\n",
    "    hour_counts = hours.value_counts()\n",
    "    hour_probs = hour_counts / len(hours)\n",
    "    features['hour_entropy'] = entropy(hour_probs)\n",
    "    features['unique_hours'] = len(hour_counts)\n",
    "    \n",
    "    days = sender_df['date'].dt.dayofweek\n",
    "    day_counts = days.value_counts()\n",
    "    day_probs = day_counts / len(days)\n",
    "    features['day_entropy'] = entropy(day_probs)\n",
    "    features['unique_days'] = len(day_counts)\n",
    "    \n",
    "    # Time consistency score (how regular are the patterns)\n",
    "    features['time_consistency_score'] = 1 - (features['hour_entropy'] / np.log(24) if features['hour_entropy'] > 0 else 0)\n",
    "    \n",
    "    # Activity period analysis\n",
    "    features['night_ratio'] = ((hours >= 22) | (hours < 6)).sum() / len(hours)\n",
    "    features['business_hours_ratio'] = ((hours >= 9) & (hours < 18)).sum() / len(hours)\n",
    "    features['morning_ratio'] = ((hours >= 6) & (hours < 9)).sum() / len(hours)\n",
    "    features['evening_ratio'] = ((hours >= 18) & (hours < 22)).sum() / len(hours)\n",
    "    \n",
    "    # Weekend activity\n",
    "    features['weekend_ratio'] = (days >= 5).sum() / len(days)\n",
    "    \n",
    "    # Minute-level patterns (bots often send at :00)\n",
    "    minutes = sender_df['date'].dt.minute\n",
    "    features['zero_minute_ratio'] = (minutes == 0).sum() / len(minutes)\n",
    "    features['round_minute_ratio'] = (minutes % 5 == 0).sum() / len(minutes)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply temporal pattern extraction\n",
    "print(\"\\n‚è≥ Extracting temporal patterns for all senders...\")\n",
    "temporal_features = []\n",
    "for sender, group in df.groupby('sender'):\n",
    "    features = calculate_temporal_patterns(group)\n",
    "    features['sender'] = sender\n",
    "    temporal_features.append(features)\n",
    "\n",
    "temporal_df = pd.DataFrame(temporal_features)\n",
    "print(f\"‚úÖ Temporal patterns extracted for {len(temporal_df)} senders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Burst Detection and Campaign Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"IMPROVED BURST DETECTION AND CAMPAIGN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def detect_bursts_and_campaigns(sender_df):\n",
    "    \"\"\"\n",
    "    Detect burst patterns and potential campaign behavior with improved thresholds\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    if len(sender_df) <= 2:\n",
    "        # Not enough data for burst detection\n",
    "        features['num_bursts'] = 0\n",
    "        features['max_burst_size'] = len(sender_df)\n",
    "        features['avg_burst_duration'] = 0\n",
    "        features['campaign_likelihood'] = 0\n",
    "        features['campaign_likelihood_relaxed'] = 0  # NEW\n",
    "        features['single_session'] = 1\n",
    "        features['adaptive_burst_score'] = 0  # NEW\n",
    "        return features\n",
    "    \n",
    "    sender_df = sender_df.sort_values('date')\n",
    "    \n",
    "    # Calculate sender's baseline inter-email time for adaptive threshold\n",
    "    intervals = sender_df['date'].diff().dropna().dt.total_seconds() / 60\n",
    "    baseline_interval = intervals.median() if len(intervals) > 0 else 60\n",
    "    \n",
    "    # Adaptive burst threshold: use sender's baseline, but cap at 30 minutes (NEW)\n",
    "    adaptive_threshold = min(max(baseline_interval * 0.5, 5), 30)  # Between 5 and 30 minutes\n",
    "    \n",
    "    # Also use fixed 30-minute threshold for campaign detection (NEW)\n",
    "    campaign_threshold = 30  # minutes\n",
    "    \n",
    "    # Identify bursts with 15-minute threshold (original)\n",
    "    burst_threshold = 15\n",
    "    bursts = []\n",
    "    current_burst = [sender_df.iloc[0]]\n",
    "    \n",
    "    for i in range(1, len(sender_df)):\n",
    "        time_diff = (sender_df.iloc[i]['date'] - sender_df.iloc[i-1]['date']).total_seconds() / 60\n",
    "        \n",
    "        if time_diff <= burst_threshold:\n",
    "            current_burst.append(sender_df.iloc[i])\n",
    "        else:\n",
    "            if len(current_burst) > 1:\n",
    "                bursts.append(pd.DataFrame(current_burst))\n",
    "            current_burst = [sender_df.iloc[i]]\n",
    "    \n",
    "    if len(current_burst) > 1:\n",
    "        bursts.append(pd.DataFrame(current_burst))\n",
    "    \n",
    "    features['num_bursts'] = len(bursts)\n",
    "    \n",
    "    # Identify campaigns with 30-minute threshold (NEW)\n",
    "    campaigns = []\n",
    "    current_campaign = [sender_df.iloc[0]]\n",
    "    \n",
    "    for i in range(1, len(sender_df)):\n",
    "        time_diff = (sender_df.iloc[i]['date'] - sender_df.iloc[i-1]['date']).total_seconds() / 60\n",
    "        \n",
    "        if time_diff <= campaign_threshold:\n",
    "            current_campaign.append(sender_df.iloc[i])\n",
    "        else:\n",
    "            if len(current_campaign) > 1:\n",
    "                campaigns.append(pd.DataFrame(current_campaign))\n",
    "            current_campaign = [sender_df.iloc[i]]\n",
    "    \n",
    "    if len(current_campaign) > 1:\n",
    "        campaigns.append(pd.DataFrame(current_campaign))\n",
    "    \n",
    "    features['num_campaigns'] = len(campaigns)  # NEW\n",
    "    \n",
    "    if len(bursts) > 0:\n",
    "        burst_sizes = [len(b) for b in bursts]\n",
    "        burst_durations = [(b['date'].max() - b['date'].min()).total_seconds() / 60 for b in bursts]\n",
    "        \n",
    "        features['max_burst_size'] = max(burst_sizes)\n",
    "        features['avg_burst_size'] = np.mean(burst_sizes)\n",
    "        features['avg_burst_duration'] = np.mean([d for d in burst_durations if d > 0]) if any(d > 0 for d in burst_durations) else 0\n",
    "        \n",
    "        # Original campaign detection: single burst with >10 emails\n",
    "        features['campaign_likelihood'] = 1 if (features['num_bursts'] == 1 and features['max_burst_size'] > 10) else 0\n",
    "        \n",
    "        # Relaxed campaign detection: single burst/campaign with ‚â•5 emails (NEW)\n",
    "        campaign_sizes = [len(c) for c in campaigns]\n",
    "        max_campaign_size = max(campaign_sizes) if len(campaign_sizes) > 0 else 0\n",
    "        features['campaign_likelihood_relaxed'] = 1 if (len(campaigns) == 1 and max_campaign_size >= 5) else 0\n",
    "    else:\n",
    "        features['max_burst_size'] = 1\n",
    "        features['avg_burst_size'] = 1\n",
    "        features['avg_burst_duration'] = 0\n",
    "        features['campaign_likelihood'] = 0\n",
    "        features['campaign_likelihood_relaxed'] = 0\n",
    "    \n",
    "    # Adaptive burst score based on sender's baseline (NEW)\n",
    "    # How many emails deviate from their typical pattern?\n",
    "    rapid_count = (intervals < adaptive_threshold).sum() if len(intervals) > 0 else 0\n",
    "    features['adaptive_burst_score'] = rapid_count / len(intervals) if len(intervals) > 0 else 0\n",
    "    \n",
    "    # Check if all emails are in a single session (< 1 hour total span)\n",
    "    total_span = (sender_df['date'].max() - sender_df['date'].min()).total_seconds() / 3600\n",
    "    features['single_session'] = 1 if total_span <= 1 else 0\n",
    "    \n",
    "    # Hit-and-run pattern (many emails in short time, then silence)\n",
    "    features['hit_and_run'] = 1 if (len(sender_df) > 10 and total_span < 0.5) else 0\n",
    "    \n",
    "    # Campaign intensity score (NEW)\n",
    "    features['max_campaign_size'] = max([len(c) for c in campaigns]) if len(campaigns) > 0 else 1\n",
    "    features['campaign_intensity'] = features['max_campaign_size'] / len(sender_df) if len(sender_df) > 0 else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply burst detection\n",
    "print(\"\\n‚è≥ Detecting bursts and campaigns with improved thresholds...\")\n",
    "burst_features = []\n",
    "for sender, group in df.groupby('sender'):\n",
    "    features = detect_bursts_and_campaigns(group)\n",
    "    features['sender'] = sender\n",
    "    burst_features.append(features)\n",
    "\n",
    "burst_df = pd.DataFrame(burst_features)\n",
    "print(f\"‚úÖ Burst analysis complete for {len(burst_df)} senders\")\n",
    "print(f\"\\nüìä Detection Statistics:\")\n",
    "print(f\"  ‚Ä¢ Campaign-like senders (strict, >10 emails): {burst_df['campaign_likelihood'].sum():,}\")\n",
    "print(f\"  ‚Ä¢ Campaign-like senders (relaxed, ‚â•5 emails): {burst_df['campaign_likelihood_relaxed'].sum():,}\")\n",
    "print(f\"  ‚Ä¢ Hit-and-run senders: {burst_df['hit_and_run'].sum():,}\")\n",
    "print(f\"  ‚Ä¢ Average campaigns per sender: {burst_df['num_campaigns'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3a: Velocity and Acceleration Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VELOCITY AND ACCELERATION FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_velocity_features(sender_df):\n",
    "    \"\"\"\n",
    "    Calculate velocity changes and acceleration in sending patterns\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    if len(sender_df) <= 2:\n",
    "        features['acceleration'] = 0\n",
    "        features['deceleration_after_burst'] = 0\n",
    "        features['velocity_variance'] = 0\n",
    "        features['max_velocity'] = 0\n",
    "        features['velocity_trend'] = 0\n",
    "        return features\n",
    "    \n",
    "    sender_df = sender_df.sort_values('date')\n",
    "    \n",
    "    # Calculate rolling velocity (emails per hour in sliding windows)\n",
    "    # Use 30-minute windows\n",
    "    window_size_minutes = 30\n",
    "    timestamps = sender_df['date'].values\n",
    "    \n",
    "    velocities = []\n",
    "    for i in range(len(sender_df)):\n",
    "        # Count emails within 30 minutes before this email\n",
    "        window_start = sender_df.iloc[i]['date'] - pd.Timedelta(minutes=window_size_minutes)\n",
    "        window_end = sender_df.iloc[i]['date']\n",
    "        emails_in_window = ((sender_df['date'] >= window_start) & (sender_df['date'] <= window_end)).sum()\n",
    "        velocity = emails_in_window / (window_size_minutes / 60)  # emails per hour\n",
    "        velocities.append(velocity)\n",
    "    \n",
    "    velocities = np.array(velocities)\n",
    "    \n",
    "    # Acceleration (rate of change in velocity)\n",
    "    if len(velocities) > 1:\n",
    "        velocity_changes = np.diff(velocities)\n",
    "        features['acceleration'] = np.mean(velocity_changes)\n",
    "        features['max_acceleration'] = np.max(velocity_changes)\n",
    "        features['velocity_variance'] = np.var(velocities)\n",
    "    else:\n",
    "        features['acceleration'] = 0\n",
    "        features['max_acceleration'] = 0\n",
    "        features['velocity_variance'] = 0\n",
    "    \n",
    "    features['max_velocity'] = np.max(velocities)\n",
    "    features['mean_velocity'] = np.mean(velocities)\n",
    "    \n",
    "    # Velocity trend (is sender speeding up or slowing down?)\n",
    "    if len(velocities) > 2:\n",
    "        from scipy.stats import linregress\n",
    "        x = np.arange(len(velocities))\n",
    "        slope, _, _, _, _ = linregress(x, velocities)\n",
    "        features['velocity_trend'] = slope  # positive = speeding up, negative = slowing down\n",
    "    else:\n",
    "        features['velocity_trend'] = 0\n",
    "    \n",
    "    # Deceleration after burst\n",
    "    # Find bursts (velocity > mean + 1 std)\n",
    "    if features['velocity_variance'] > 0:\n",
    "        burst_threshold = features['mean_velocity'] + np.sqrt(features['velocity_variance'])\n",
    "        burst_indices = np.where(velocities > burst_threshold)[0]\n",
    "        \n",
    "        if len(burst_indices) > 0:\n",
    "            decelerations = []\n",
    "            for burst_idx in burst_indices:\n",
    "                # Look at next 3 timestamps (if they exist)\n",
    "                if burst_idx + 3 < len(velocities):\n",
    "                    post_burst_velocity = velocities[burst_idx + 1:burst_idx + 4].mean()\n",
    "                    decel = velocities[burst_idx] - post_burst_velocity\n",
    "                    decelerations.append(decel)\n",
    "            \n",
    "            features['deceleration_after_burst'] = np.mean(decelerations) if decelerations else 0\n",
    "        else:\n",
    "            features['deceleration_after_burst'] = 0\n",
    "    else:\n",
    "        features['deceleration_after_burst'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply velocity feature extraction\n",
    "print(\"\\n‚è≥ Extracting velocity and acceleration features...\")\n",
    "velocity_features = []\n",
    "for sender, group in df.groupby('sender'):\n",
    "    features = calculate_velocity_features(group)\n",
    "    features['sender'] = sender\n",
    "    velocity_features.append(features)\n",
    "\n",
    "velocity_df = pd.DataFrame(velocity_features)\n",
    "print(f\"‚úÖ Velocity features extracted for {len(velocity_df)} senders\")\n",
    "print(f\"üìä New features: {[col for col in velocity_df.columns if col != 'sender']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3b: Pattern Irregularity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PATTERN IRREGULARITY FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_pattern_irregularity(sender_df):\n",
    "    \"\"\"\n",
    "    Measure how irregular/unpredictable the sending pattern is\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    if len(sender_df) <= 3:\n",
    "        features['jitter_score'] = 0\n",
    "        features['pattern_break_count'] = 0\n",
    "        features['median_absolute_deviation'] = 0\n",
    "        features['burstiness_index'] = 0\n",
    "        return features\n",
    "    \n",
    "    sender_df = sender_df.sort_values('date')\n",
    "    \n",
    "    # Inter-email intervals in minutes\n",
    "    intervals = sender_df['date'].diff().dropna().dt.total_seconds() / 60\n",
    "    \n",
    "    if len(intervals) > 0:\n",
    "        # Jitter score: normalized standard deviation of intervals\n",
    "        mean_interval = intervals.mean()\n",
    "        if mean_interval > 0:\n",
    "            features['jitter_score'] = intervals.std() / mean_interval\n",
    "        else:\n",
    "            features['jitter_score'] = 0\n",
    "        \n",
    "        # Median Absolute Deviation (robust measure of variability)\n",
    "        median_interval = intervals.median()\n",
    "        features['median_absolute_deviation'] = np.median(np.abs(intervals - median_interval))\n",
    "        \n",
    "        # Pattern breaks: count sudden changes in interval patterns\n",
    "        # A break is when interval changes by more than 2x\n",
    "        if len(intervals) > 1:\n",
    "            interval_ratios = intervals.iloc[1:].values / intervals.iloc[:-1].values\n",
    "            # Count breaks (ratio > 2 or < 0.5)\n",
    "            breaks = ((interval_ratios > 2) | (interval_ratios < 0.5)).sum()\n",
    "            features['pattern_break_count'] = breaks\n",
    "            features['pattern_break_ratio'] = breaks / len(interval_ratios) if len(interval_ratios) > 0 else 0\n",
    "        else:\n",
    "            features['pattern_break_count'] = 0\n",
    "            features['pattern_break_ratio'] = 0\n",
    "        \n",
    "        # Burstiness index (Goh & Barab√°si measure)\n",
    "        # B = (œÉ - Œº) / (œÉ + Œº) where œÉ is std and Œº is mean\n",
    "        # B = -1 (periodic), B = 0 (random/Poisson), B = 1 (bursty)\n",
    "        sigma = intervals.std()\n",
    "        mu = intervals.mean()\n",
    "        if (sigma + mu) > 0:\n",
    "            features['burstiness_index'] = (sigma - mu) / (sigma + mu)\n",
    "        else:\n",
    "            features['burstiness_index'] = 0\n",
    "    else:\n",
    "        features['jitter_score'] = 0\n",
    "        features['median_absolute_deviation'] = 0\n",
    "        features['pattern_break_count'] = 0\n",
    "        features['pattern_break_ratio'] = 0\n",
    "        features['burstiness_index'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply pattern irregularity extraction\n",
    "print(\"\\n‚è≥ Extracting pattern irregularity features...\")\n",
    "irregularity_features = []\n",
    "for sender, group in df.groupby('sender'):\n",
    "    features = calculate_pattern_irregularity(group)\n",
    "    features['sender'] = sender\n",
    "    irregularity_features.append(features)\n",
    "\n",
    "irregularity_df = pd.DataFrame(irregularity_features)\n",
    "print(f\"‚úÖ Pattern irregularity features extracted for {len(irregularity_df)} senders\")\n",
    "print(f\"üìä New features: {[col for col in irregularity_df.columns if col != 'sender']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3c: Session-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SESSION-BASED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_session_features(sender_df):\n",
    "    \"\"\"\n",
    "    Identify distinct sending sessions and extract session-based features\n",
    "    Session = continuous period of activity (gap < 60 minutes = same session)\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    if len(sender_df) <= 1:\n",
    "        features['sessions_count'] = 1\n",
    "        features['avg_session_length'] = 0\n",
    "        features['avg_session_emails'] = len(sender_df)\n",
    "        features['session_gap_variance'] = 0\n",
    "        features['max_session_length'] = 0\n",
    "        features['longest_session_emails'] = len(sender_df)\n",
    "        return features\n",
    "    \n",
    "    sender_df = sender_df.sort_values('date')\n",
    "    \n",
    "    # Define session gap threshold (60 minutes = new session)\n",
    "    session_gap_threshold = 60  # minutes\n",
    "    \n",
    "    # Identify sessions\n",
    "    sessions = []\n",
    "    current_session = [sender_df.iloc[0]]\n",
    "    \n",
    "    for i in range(1, len(sender_df)):\n",
    "        time_gap = (sender_df.iloc[i]['date'] - sender_df.iloc[i-1]['date']).total_seconds() / 60\n",
    "        \n",
    "        if time_gap <= session_gap_threshold:\n",
    "            current_session.append(sender_df.iloc[i])\n",
    "        else:\n",
    "            sessions.append(pd.DataFrame(current_session))\n",
    "            current_session = [sender_df.iloc[i]]\n",
    "    \n",
    "    # Don't forget last session\n",
    "    sessions.append(pd.DataFrame(current_session))\n",
    "    \n",
    "    # Calculate session features\n",
    "    features['sessions_count'] = len(sessions)\n",
    "    \n",
    "    session_lengths = []  # in minutes\n",
    "    session_email_counts = []\n",
    "    \n",
    "    for session in sessions:\n",
    "        if len(session) > 1:\n",
    "            session_length = (session['date'].max() - session['date'].min()).total_seconds() / 60\n",
    "        else:\n",
    "            session_length = 0\n",
    "        session_lengths.append(session_length)\n",
    "        session_email_counts.append(len(session))\n",
    "    \n",
    "    features['avg_session_length'] = np.mean(session_lengths)\n",
    "    features['max_session_length'] = np.max(session_lengths)\n",
    "    features['avg_session_emails'] = np.mean(session_email_counts)\n",
    "    features['max_session_emails'] = np.max(session_email_counts)\n",
    "    features['longest_session_emails'] = session_email_counts[np.argmax(session_lengths)]\n",
    "    \n",
    "    # Session gaps (time between sessions)\n",
    "    if len(sessions) > 1:\n",
    "        session_gaps = []\n",
    "        for i in range(len(sessions) - 1):\n",
    "            gap = (sessions[i+1]['date'].min() - sessions[i]['date'].max()).total_seconds() / 60\n",
    "            session_gaps.append(gap)\n",
    "        \n",
    "        features['avg_session_gap'] = np.mean(session_gaps)\n",
    "        features['session_gap_variance'] = np.var(session_gaps)\n",
    "        features['max_session_gap'] = np.max(session_gaps)\n",
    "        features['min_session_gap'] = np.min(session_gaps)\n",
    "    else:\n",
    "        features['avg_session_gap'] = 0\n",
    "        features['session_gap_variance'] = 0\n",
    "        features['max_session_gap'] = 0\n",
    "        features['min_session_gap'] = 0\n",
    "    \n",
    "    # Session consistency\n",
    "    features['session_length_cv'] = np.std(session_lengths) / np.mean(session_lengths) if np.mean(session_lengths) > 0 else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply session feature extraction\n",
    "print(\"\\n‚è≥ Extracting session-based features...\")\n",
    "session_features = []\n",
    "for sender, group in df.groupby('sender'):\n",
    "    features = calculate_session_features(group)\n",
    "    features['sender'] = sender\n",
    "    session_features.append(features)\n",
    "\n",
    "session_df = pd.DataFrame(session_features)\n",
    "print(f\"‚úÖ Session features extracted for {len(session_df)} senders\")\n",
    "print(f\"üìä New features: {[col for col in session_df.columns if col != 'sender']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RELATIVE TIME AND FRONT-LOADING FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_relative_time_features(sender_df):\n",
    "    \"\"\"\n",
    "    Calculate features based on relative timing and activity decay\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    if len(sender_df) <= 1:\n",
    "        features['percent_emails_in_first_hour'] = 1.0\n",
    "        features['percent_emails_in_first_day'] = 1.0\n",
    "        features['decay_rate'] = 0\n",
    "        features['front_loading_score'] = 1.0\n",
    "        features['activity_concentration'] = 1.0\n",
    "        return features\n",
    "    \n",
    "    sender_df = sender_df.sort_values('date')\n",
    "    \n",
    "    # Total timespan\n",
    "    first_email = sender_df['date'].min()\n",
    "    last_email = sender_df['date'].max()\n",
    "    total_span_hours = (last_email - first_email).total_seconds() / 3600\n",
    "    \n",
    "    # Front-loading: what percentage of emails are in the first hour/day?\n",
    "    if total_span_hours >= 1:\n",
    "        first_hour_cutoff = first_email + pd.Timedelta(hours=1)\n",
    "        emails_in_first_hour = (sender_df['date'] <= first_hour_cutoff).sum()\n",
    "        features['percent_emails_in_first_hour'] = emails_in_first_hour / len(sender_df)\n",
    "    else:\n",
    "        features['percent_emails_in_first_hour'] = 1.0\n",
    "    \n",
    "    if total_span_hours >= 24:\n",
    "        first_day_cutoff = first_email + pd.Timedelta(days=1)\n",
    "        emails_in_first_day = (sender_df['date'] <= first_day_cutoff).sum()\n",
    "        features['percent_emails_in_first_day'] = emails_in_first_day / len(sender_df)\n",
    "    else:\n",
    "        features['percent_emails_in_first_day'] = 1.0\n",
    "    \n",
    "    # Decay rate: fit exponential decay model to email activity over time\n",
    "    if len(sender_df) > 5 and total_span_hours > 1:\n",
    "        # Divide timeline into bins\n",
    "        num_bins = min(10, len(sender_df))\n",
    "        bins = pd.cut(sender_df['date'], bins=num_bins)\n",
    "        email_counts = sender_df.groupby(bins).size().values\n",
    "        \n",
    "        # Fit exponential decay: y = a * exp(b * x)\n",
    "        # Log transform: log(y) = log(a) + b * x\n",
    "        x = np.arange(len(email_counts))\n",
    "        y = email_counts + 1  # Add 1 to avoid log(0)\n",
    "        \n",
    "        try:\n",
    "            from scipy.stats import linregress\n",
    "            slope, _, _, _, _ = linregress(x, np.log(y))\n",
    "            features['decay_rate'] = slope  # negative = decay, positive = growth\n",
    "        except:\n",
    "            features['decay_rate'] = 0\n",
    "    else:\n",
    "        features['decay_rate'] = 0\n",
    "    \n",
    "    # Front-loading score (Gini coefficient of email distribution over time)\n",
    "    # Measures inequality: 1 = all emails at once, 0 = perfectly spread out\n",
    "    if len(sender_df) > 2 and total_span_hours > 0:\n",
    "        # Cumulative distribution\n",
    "        relative_times = (sender_df['date'] - first_email).dt.total_seconds()\n",
    "        \n",
    "        # Check if all emails are at the same time\n",
    "        if relative_times.max() > 0:\n",
    "            relative_times = relative_times / relative_times.max()\n",
    "            \n",
    "            # Gini coefficient\n",
    "            n = len(relative_times)\n",
    "            sum_times = sum(relative_times)\n",
    "            \n",
    "            # Add check for zero sum (all times are 0)\n",
    "            if sum_times > 0:\n",
    "                gini = (2 * sum((i + 1) * val for i, val in enumerate(sorted(relative_times))) / (n * sum_times)) - (n + 1) / n\n",
    "                features['front_loading_score'] = gini\n",
    "            else:\n",
    "                features['front_loading_score'] = 1.0  # All at once = maximum front-loading\n",
    "        else:\n",
    "            features['front_loading_score'] = 1.0  # All emails at same time\n",
    "    else:\n",
    "        features['front_loading_score'] = 1.0\n",
    "    \n",
    "    # Activity concentration: what fraction of total time contains 80% of emails?\n",
    "    if total_span_hours > 0 and len(sender_df) > 1:\n",
    "        target_emails = max(1, int(0.8 * len(sender_df)))\n",
    "        # Find the smallest time window containing 80% of emails\n",
    "        min_window = total_span_hours\n",
    "        for i in range(len(sender_df) - target_emails + 1):\n",
    "            window_start = sender_df.iloc[i]['date']\n",
    "            window_end = sender_df.iloc[i + target_emails - 1]['date']\n",
    "            window_size = (window_end - window_start).total_seconds() / 3600\n",
    "            min_window = min(min_window, window_size)\n",
    "        \n",
    "        features['activity_concentration'] = min_window / total_span_hours if total_span_hours > 0 else 1.0\n",
    "    else:\n",
    "        features['activity_concentration'] = 1.0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply relative time feature extraction\n",
    "print(\"\\n‚è≥ Extracting relative time and front-loading features...\")\n",
    "relative_time_features = []\n",
    "for sender, group in df.groupby('sender'):\n",
    "    features = calculate_relative_time_features(group)\n",
    "    features['sender'] = sender\n",
    "    relative_time_features.append(features)\n",
    "\n",
    "relative_time_df = pd.DataFrame(relative_time_features)\n",
    "print(f\"‚úÖ Relative time features extracted for {len(relative_time_df)} senders\")\n",
    "print(f\"üìä New features: {[col for col in relative_time_df.columns if col != 'sender']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Combine All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMBINING ALL TEMPORAL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Merge all feature dataframes\n",
    "print(\"‚è≥ Merging feature sets...\")\n",
    "sender_features = freq_df.merge(temporal_df, on='sender', suffixes=('', '_temp'))\n",
    "sender_features = sender_features.merge(burst_df, on='sender', suffixes=('', '_burst'))\n",
    "sender_features = sender_features.merge(velocity_df, on='sender', suffixes=('', '_velocity'))  # NEW\n",
    "sender_features = sender_features.merge(irregularity_df, on='sender', suffixes=('', '_irreg'))  # NEW\n",
    "sender_features = sender_features.merge(session_df, on='sender', suffixes=('', '_session'))  # NEW\n",
    "sender_features = sender_features.merge(relative_time_df, on='sender', suffixes=('', '_reltime'))  # NEW\n",
    "\n",
    "# Remove duplicate label columns if any\n",
    "label_cols = [col for col in sender_features.columns if 'label' in col]\n",
    "if len(label_cols) > 1:\n",
    "    sender_features['label'] = sender_features[label_cols[0]]\n",
    "    for col in label_cols[1:]:\n",
    "        if col in sender_features.columns:\n",
    "            sender_features = sender_features.drop(columns=[col])\n",
    "\n",
    "print(f\"\\n‚úÖ Combined feature matrix created\")\n",
    "print(f\"  ‚Ä¢ Shape: {sender_features.shape}\")\n",
    "print(f\"  ‚Ä¢ Features: {sender_features.shape[1] - 2} (excluding sender and label)\")\n",
    "print(f\"  ‚Ä¢ Phishing senders: {(sender_features['label']==1).sum():,}\")\n",
    "print(f\"  ‚Ä¢ Legitimate senders: {(sender_features['label']==0).sum():,}\")\n",
    "\n",
    "# Display feature list\n",
    "feature_cols = [col for col in sender_features.columns if col not in ['sender', 'label']]\n",
    "print(f\"\\nüìä Features created ({len(feature_cols)} total):\")\n",
    "\n",
    "# Group features by category for better readability\n",
    "freq_features = [c for c in feature_cols if any(x in c for x in ['per_minute', 'per_5min', 'per_15min', 'per_30min', 'per_hour', 'per_day', 'spikiness', 'concentration'])]\n",
    "velocity_features = [c for c in feature_cols if any(x in c for x in ['velocity', 'acceleration', 'deceleration'])]\n",
    "pattern_features = [c for c in feature_cols if any(x in c for x in ['jitter', 'pattern_break', 'burstiness', 'median_absolute'])]\n",
    "session_features = [c for c in feature_cols if 'session' in c]\n",
    "time_features = [c for c in feature_cols if any(x in c for x in ['percent_emails', 'decay', 'front_loading', 'concentration'])]\n",
    "other_features = [c for c in feature_cols if c not in freq_features + velocity_features + pattern_features + session_features + time_features]\n",
    "\n",
    "print(f\"\\n  üìà Frequency features ({len(freq_features)}):\")\n",
    "for i in range(0, len(freq_features), 3):\n",
    "    print(\"     \", \", \".join(freq_features[i:i+3]))\n",
    "\n",
    "print(f\"\\n  üöÄ Velocity & Acceleration features ({len(velocity_features)}):\")\n",
    "for i in range(0, len(velocity_features), 3):\n",
    "    print(\"     \", \", \".join(velocity_features[i:i+3]))\n",
    "\n",
    "print(f\"\\n  üìä Pattern Irregularity features ({len(pattern_features)}):\")\n",
    "for i in range(0, len(pattern_features), 3):\n",
    "    print(\"     \", \", \".join(pattern_features[i:i+3]))\n",
    "\n",
    "print(f\"\\n  üîÑ Session-Based features ({len(session_features)}):\")\n",
    "for i in range(0, len(session_features), 3):\n",
    "    print(\"     \", \", \".join(session_features[i:i+3]))\n",
    "\n",
    "print(f\"\\n  ‚è∞ Relative Time features ({len(time_features)}):\")\n",
    "for i in range(0, len(time_features), 3):\n",
    "    print(\"     \", \", \".join(time_features[i:i+3]))\n",
    "\n",
    "print(f\"\\n  üîß Other features ({len(other_features)}):\")\n",
    "for i in range(0, len(other_features), 4):\n",
    "    print(\"     \", \", \".join(other_features[i:i+4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Statistical Comparison - Phishing vs Legitimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STATISTICAL COMPARISON: PHISHING VS LEGITIMATE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Separate by label\n",
    "phishing_senders = sender_features[sender_features['label'] == 1]\n",
    "legitimate_senders = sender_features[sender_features['label'] == 0]\n",
    "\n",
    "print(f\"\\nüìä Class Distribution:\")\n",
    "print(f\"  ‚Ä¢ Phishing senders: {len(phishing_senders):,} ({len(phishing_senders)/len(sender_features)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Legitimate senders: {len(legitimate_senders):,} ({len(legitimate_senders)/len(sender_features)*100:.1f}%)\")\n",
    "\n",
    "# Statistical comparison table\n",
    "comparison_results = []\n",
    "\n",
    "for feature in feature_cols:\n",
    "    phish_mean = phishing_senders[feature].mean()\n",
    "    legit_mean = legitimate_senders[feature].mean()\n",
    "    \n",
    "    phish_std = phishing_senders[feature].std()\n",
    "    legit_std = legitimate_senders[feature].std()\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, p_value = stats.ttest_ind(\n",
    "        phishing_senders[feature].dropna(),\n",
    "        legitimate_senders[feature].dropna(),\n",
    "        equal_var=False\n",
    "    )\n",
    "    \n",
    "    # Cohen's d effect size\n",
    "    pooled_std = np.sqrt((phish_std**2 + legit_std**2) / 2)\n",
    "    cohens_d = (phish_mean - legit_mean) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    # Calculate AUC if possible\n",
    "    try:\n",
    "        combined = pd.concat([\n",
    "            phishing_senders[[feature]].assign(label=1),\n",
    "            legitimate_senders[[feature]].assign(label=0)\n",
    "        ]).dropna()\n",
    "        \n",
    "        if len(combined) > 0 and combined[feature].nunique() > 1:\n",
    "            auc = roc_auc_score(combined['label'], combined[feature])\n",
    "            # Correct AUC if needed (should be > 0.5 for good features)\n",
    "            auc = max(auc, 1-auc)\n",
    "        else:\n",
    "            auc = 0.5\n",
    "    except:\n",
    "        auc = 0.5\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Feature': feature,\n",
    "        'Phishing_Mean': phish_mean,\n",
    "        'Legitimate_Mean': legit_mean,\n",
    "        'P_Value': p_value,\n",
    "        'Cohens_D': abs(cohens_d),\n",
    "        'AUC': auc\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "comparison_df = comparison_df.sort_values('AUC', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 15 Most Discriminative Features (by AUC):\")\n",
    "print(\"=\"*100)\n",
    "cohens_d_label = \"Cohen's D\"\n",
    "print(f\"{'Feature':<30} {'Phishing':<12} {'Legitimate':<12} {'P-Value':<10} {cohens_d_label:<10} {'AUC':<8}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for _, row in comparison_df.head(15).iterrows():\n",
    "    sig = '***' if row['P_Value'] < 0.001 else '**' if row['P_Value'] < 0.01 else '*' if row['P_Value'] < 0.05 else ''\n",
    "    print(f\"{row['Feature']:<30} {row['Phishing_Mean']:<12.4f} {row['Legitimate_Mean']:<12.4f} \"\n",
    "          f\"{row['P_Value']:<10.4f} {row['Cohens_D']:<10.4f} {row['AUC']:<8.4f} {sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Frequency Pattern Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FREQUENCY PATTERN VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select top frequency features for visualization\n",
    "freq_features = ['emails_per_minute_max', 'emails_per_5min_max', 'emails_per_hour_max', \n",
    "                 'burst_ratio', 'concentration_1hr']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Frequency-Based Features: Phishing vs Legitimate', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, feature in enumerate(freq_features):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Create log-scale for better visualization\n",
    "    phish_data = phishing_senders[feature].clip(lower=0.1)\n",
    "    legit_data = legitimate_senders[feature].clip(lower=0.1)\n",
    "    \n",
    "    # Plot distributions\n",
    "    ax.hist(np.log10(phish_data), bins=30, alpha=0.5, label='Phishing', color='red', density=True)\n",
    "    ax.hist(np.log10(legit_data), bins=30, alpha=0.5, label='Legitimate', color='green', density=True)\n",
    "    \n",
    "    ax.set_xlabel(f'Log10({feature})')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(feature.replace('_', ' ').title())\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add AUC score\n",
    "    feature_auc = comparison_df[comparison_df['Feature'] == feature]['AUC'].values[0]\n",
    "    ax.text(0.95, 0.95, f'AUC: {feature_auc:.3f}', transform=ax.transAxes, \n",
    "            ha='right', va='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Frequency pattern visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Burst Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BURST PATTERN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze burst patterns\n",
    "burst_senders = sender_features[sender_features['is_burst_sender'] == 1]\n",
    "campaign_senders = sender_features[sender_features['campaign_likelihood'] == 1]\n",
    "hit_run_senders = sender_features[sender_features['hit_and_run'] == 1]\n",
    "\n",
    "print(\"\\nüìä Burst Pattern Statistics:\")\n",
    "print(f\"  ‚Ä¢ Burst senders: {len(burst_senders):,} ({len(burst_senders)/len(sender_features)*100:.1f}%)\")\n",
    "print(f\"    - Phishing: {(burst_senders['label']==1).sum():,} ({(burst_senders['label']==1).mean()*100:.1f}%)\")\n",
    "print(f\"    - Legitimate: {(burst_senders['label']==0).sum():,} ({(burst_senders['label']==0).mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n  ‚Ä¢ Campaign-like senders: {len(campaign_senders):,} ({len(campaign_senders)/len(sender_features)*100:.1f}%)\")\n",
    "print(f\"    - Phishing: {(campaign_senders['label']==1).sum():,} ({(campaign_senders['label']==1).mean()*100:.1f}%)\")\n",
    "print(f\"    - Legitimate: {(campaign_senders['label']==0).sum():,} ({(campaign_senders['label']==0).mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n  ‚Ä¢ Hit-and-run senders: {len(hit_run_senders):,} ({len(hit_run_senders)/len(sender_features)*100:.1f}%)\")\n",
    "print(f\"    - Phishing: {(hit_run_senders['label']==1).sum():,} ({(hit_run_senders['label']==1).mean()*100:.1f}%)\")\n",
    "print(f\"    - Legitimate: {(hit_run_senders['label']==0).sum():,} ({(hit_run_senders['label']==0).mean()*100:.1f}%)\")\n",
    "\n",
    "# Visualize burst characteristics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Burst size distribution\n",
    "axes[0].boxplot([\n",
    "    phishing_senders['max_burst_size'].dropna(),\n",
    "    legitimate_senders['max_burst_size'].dropna()\n",
    "], labels=['Phishing', 'Legitimate'])\n",
    "axes[0].set_ylabel('Max Burst Size')\n",
    "axes[0].set_title('Maximum Burst Size Comparison')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Number of bursts\n",
    "axes[1].boxplot([\n",
    "    phishing_senders['num_bursts'].dropna(),\n",
    "    legitimate_senders['num_bursts'].dropna()\n",
    "], labels=['Phishing', 'Legitimate'])\n",
    "axes[1].set_ylabel('Number of Bursts')\n",
    "axes[1].set_title('Burst Count Comparison')\n",
    "\n",
    "# Burst duration\n",
    "axes[2].boxplot([\n",
    "    phishing_senders['avg_burst_duration'].dropna(),\n",
    "    legitimate_senders['avg_burst_duration'].dropna()\n",
    "], labels=['Phishing', 'Legitimate'])\n",
    "axes[2].set_ylabel('Avg Burst Duration (minutes)')\n",
    "axes[2].set_title('Average Burst Duration')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Burst pattern analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Temporal Entropy and Randomness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEMPORAL ENTROPY AND RANDOMNESS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Entropy comparison\n",
    "entropy_features = ['hour_entropy', 'day_entropy', 'time_consistency_score']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Temporal Entropy: Phishing vs Legitimate', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, feature in enumerate(entropy_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Violin plot for better distribution visualization\n",
    "    data = [\n",
    "        phishing_senders[feature].dropna(),\n",
    "        legitimate_senders[feature].dropna()\n",
    "    ]\n",
    "    \n",
    "    parts = ax.violinplot(data, positions=[1, 2], widths=0.6, showmeans=True, showmedians=True)\n",
    "    \n",
    "    # Color the violin plots\n",
    "    colors = ['red', 'green']\n",
    "    for pc, color in zip(parts['bodies'], colors):\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_alpha(0.3)\n",
    "    \n",
    "    ax.set_xticks([1, 2])\n",
    "    ax.set_xticklabels(['Phishing', 'Legitimate'])\n",
    "    ax.set_ylabel(feature.replace('_', ' ').title())\n",
    "    ax.set_title(feature.replace('_', ' ').title())\n",
    "    \n",
    "    # Add mean values\n",
    "    phish_mean = phishing_senders[feature].mean()\n",
    "    legit_mean = legitimate_senders[feature].mean()\n",
    "    ax.text(1, ax.get_ylim()[1] * 0.95, f'Œº={phish_mean:.3f}', ha='center')\n",
    "    ax.text(2, ax.get_ylim()[1] * 0.95, f'Œº={legit_mean:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bot-like behavior analysis\n",
    "print(\"\\nü§ñ Bot-like Behavior Indicators:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Senders with zero minute preference (likely automated)\n",
    "zero_minute_senders = sender_features[sender_features['zero_minute_ratio'] > 0.8]\n",
    "print(f\"\\nSenders with >80% emails at :00 minute:\")\n",
    "print(f\"  Total: {len(zero_minute_senders):,}\")\n",
    "print(f\"  Phishing: {(zero_minute_senders['label']==1).sum():,} ({(zero_minute_senders['label']==1).mean()*100:.1f}%)\")\n",
    "\n",
    "# Senders with perfect regularity (low entropy)\n",
    "low_entropy = sender_features[sender_features['hour_entropy'] < 0.5]\n",
    "print(f\"\\nSenders with very low hour entropy (<0.5):\")\n",
    "print(f\"  Total: {len(low_entropy):,}\")\n",
    "print(f\"  Phishing: {(low_entropy['label']==1).sum():,} ({(low_entropy['label']==1).mean()*100:.1f}%)\")\n",
    "\n",
    "# Round minute preference\n",
    "round_minute = sender_features[sender_features['round_minute_ratio'] > 0.9]\n",
    "print(f\"\\nSenders with >90% emails at round minutes (:00, :05, :10, etc):\")\n",
    "print(f\"  Total: {len(round_minute):,}\")\n",
    "print(f\"  Phishing: {(round_minute['label']==1).sum():,} ({(round_minute['label']==1).mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Entropy analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Finding Optimal Time Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINDING OPTIMAL TIME WINDOWS FOR DISCRIMINATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test different window sizes\n",
    "window_features = [\n",
    "    'emails_per_minute_max',\n",
    "    'emails_per_5min_max', \n",
    "    'emails_per_15min_max',\n",
    "    'emails_per_hour_max',\n",
    "    'emails_per_day_max'\n",
    "]\n",
    "\n",
    "window_names = ['1 min', '5 min', '15 min', '1 hour', '1 day']\n",
    "\n",
    "window_performance = []\n",
    "for feature, name in zip(window_features, window_names):\n",
    "    auc = comparison_df[comparison_df['Feature'] == feature]['AUC'].values[0]\n",
    "    cohens_d = comparison_df[comparison_df['Feature'] == feature]['Cohens_D'].values[0]\n",
    "    window_performance.append({\n",
    "        'Window': name,\n",
    "        'Feature': feature,\n",
    "        'AUC': auc,\n",
    "        'Cohens_D': cohens_d\n",
    "    })\n",
    "\n",
    "window_df = pd.DataFrame(window_performance)\n",
    "\n",
    "# Visualize window performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# AUC by window size\n",
    "ax1.bar(window_df['Window'], window_df['AUC'], color='steelblue', edgecolor='black')\n",
    "ax1.set_xlabel('Time Window')\n",
    "ax1.set_ylabel('AUC Score')\n",
    "ax1.set_title('Discrimination Power by Time Window')\n",
    "ax1.set_ylim([0.5, max(window_df['AUC']) * 1.1])\n",
    "ax1.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Random')\n",
    "ax1.axhline(y=0.7, color='g', linestyle='--', alpha=0.5, label='Good')\n",
    "ax1.legend()\n",
    "\n",
    "# Effect size by window\n",
    "ax2.bar(window_df['Window'], window_df['Cohens_D'], color='coral', edgecolor='black')\n",
    "ax2.set_xlabel('Time Window')\n",
    "ax2.set_ylabel(\"Cohen's D (Effect Size)\")\n",
    "ax2.set_title('Effect Size by Time Window')\n",
    "ax2.axhline(y=0.2, color='r', linestyle='--', alpha=0.5, label='Small')\n",
    "ax2.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Medium')\n",
    "ax2.axhline(y=0.8, color='g', linestyle='--', alpha=0.5, label='Large')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Optimal Time Windows:\")\n",
    "print(\"=\"*50)\n",
    "print(window_df.sort_values('AUC', ascending=False).to_string(index=False))\n",
    "\n",
    "best_window = window_df.sort_values('AUC', ascending=False).iloc[0]\n",
    "print(f\"\\nüéØ Best performing window: {best_window['Window']} (AUC = {best_window['AUC']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Feature Correlation and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE CORRELATION AND SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select numerical features only\n",
    "numerical_features = sender_features[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = sender_features[numerical_features].corr()\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature1': corr_matrix.columns[i],\n",
    "                'Feature2': corr_matrix.columns[j],\n",
    "                'Correlation': corr_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"\\n‚ö†Ô∏è Highly Correlated Feature Pairs (|r| > 0.8):\")\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', key=abs, ascending=False)\n",
    "    print(high_corr_df.to_string(index=False))\n",
    "    print(\"\\nüí° Consider removing one feature from each highly correlated pair\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No highly correlated feature pairs found (|r| > 0.8)\")\n",
    "\n",
    "# Feature importance based on multiple metrics\n",
    "feature_importance = comparison_df[['Feature', 'AUC', 'Cohens_D']].copy()\n",
    "feature_importance['Combined_Score'] = (feature_importance['AUC'] - 0.5) * 2 + feature_importance['Cohens_D']\n",
    "feature_importance = feature_importance.sort_values('Combined_Score', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ Top 20 Features by Combined Importance Score:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Rank':<6} {'Feature':<30} {'AUC':<10} {'Effect Size':<12} {'Score':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for idx, row in feature_importance.head(20).iterrows():\n",
    "    rank = list(feature_importance.index).index(idx) + 1\n",
    "    print(f\"{rank:<6} {row['Feature']:<30} {row['AUC']:<10.4f} {row['Cohens_D']:<12.4f} {row['Combined_Score']:<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Rule-Based Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RULE-BASED INSIGHTS FOR QUICK FILTERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find thresholds for high-confidence predictions\n",
    "def find_optimal_threshold(feature_data, labels, target_precision=0.9):\n",
    "    \"\"\"\n",
    "    Find threshold that gives target precision for phishing detection\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid_idx = ~feature_data.isna()\n",
    "    feature_data = feature_data[valid_idx]\n",
    "    labels = labels[valid_idx]\n",
    "    \n",
    "    if len(feature_data) == 0 or feature_data.nunique() <= 1:\n",
    "        return None, None, None\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(labels, feature_data)\n",
    "    \n",
    "    # Find threshold closest to target precision\n",
    "    valid_idx = precision >= target_precision\n",
    "    if valid_idx.any():\n",
    "        idx = np.where(valid_idx)[0][0]\n",
    "        return thresholds[idx] if idx < len(thresholds) else None, precision[idx], recall[idx]\n",
    "    return None, None, None\n",
    "\n",
    "print(\"\\nüéØ High-Confidence Rules for Phishing Detection (‚â•90% Precision):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test key features for rules\n",
    "rule_features = [\n",
    "    ('emails_per_5min_max', 'greater'),\n",
    "    ('burst_ratio', 'greater'),\n",
    "    ('night_ratio', 'greater'),\n",
    "    ('hour_entropy', 'less'),\n",
    "    ('time_consistency_score', 'greater'),\n",
    "    ('campaign_likelihood', 'equal'),\n",
    "    ('hit_and_run', 'equal')\n",
    "]\n",
    "\n",
    "rules = []\n",
    "for feature, direction in rule_features:\n",
    "    if feature not in sender_features.columns:\n",
    "        continue\n",
    "        \n",
    "    feature_data = sender_features[feature].copy()\n",
    "    labels = sender_features['label'].copy()\n",
    "    \n",
    "    # Adjust for direction\n",
    "    if direction == 'less':\n",
    "        feature_data = -feature_data\n",
    "    \n",
    "    threshold, precision, recall = find_optimal_threshold(feature_data, labels, 0.9)\n",
    "    \n",
    "    if threshold is not None:\n",
    "        if direction == 'less':\n",
    "            threshold = -threshold\n",
    "        \n",
    "        # Calculate coverage\n",
    "        if direction == 'greater':\n",
    "            coverage = (sender_features[feature] > threshold).mean()\n",
    "        elif direction == 'less':\n",
    "            coverage = (sender_features[feature] < threshold).mean()\n",
    "        else:  # equal\n",
    "            coverage = (sender_features[feature] == 1).mean()\n",
    "        \n",
    "        rules.append({\n",
    "            'Feature': feature,\n",
    "            'Operator': direction,\n",
    "            'Threshold': threshold,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'Coverage': coverage\n",
    "        })\n",
    "\n",
    "if rules:\n",
    "    rules_df = pd.DataFrame(rules).sort_values('Recall', ascending=False)\n",
    "    \n",
    "    for _, rule in rules_df.iterrows():\n",
    "        op_symbol = '>' if rule['Operator'] == 'greater' else '<' if rule['Operator'] == 'less' else '=='\n",
    "        print(f\"\\nüìå Rule: {rule['Feature']} {op_symbol} {rule['Threshold']:.3f}\")\n",
    "        print(f\"   ‚Üí Precision: {rule['Precision']*100:.1f}%\")\n",
    "        print(f\"   ‚Üí Recall: {rule['Recall']*100:.1f}%\")\n",
    "        print(f\"   ‚Üí Coverage: {rule['Coverage']*100:.1f}% of all senders\")\n",
    "\n",
    "print(\"\\nüí° These rules can be used for immediate filtering or as features in your model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Export Final Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPORTING FINAL FEATURE MATRIX\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ==================== EXPORT CONTROL ====================\n",
    "# Set this to True to export files, False to skip export\n",
    "EXPORT_ENABLED = True\n",
    "# ========================================================\n",
    "\n",
    "if not EXPORT_ENABLED:\n",
    "    print(\"\\n‚ö†Ô∏è  EXPORT DISABLED - Set EXPORT_ENABLED = True to export files\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    # Select top features based on importance\n",
    "    top_features = feature_importance.head(25)['Feature'].tolist()\n",
    "\n",
    "    # Ensure we have sender and label columns\n",
    "    export_columns = ['sender'] + top_features + ['label']\n",
    "    export_columns = [col for col in export_columns if col in sender_features.columns]\n",
    "\n",
    "    # Create final feature matrix\n",
    "    final_features = sender_features[export_columns].copy()\n",
    "\n",
    "    # Save to CSV - notebook is already in notebooks/ directory, so just use filename\n",
    "    output_filename = 'sender_temporal_features_final.csv'\n",
    "    final_features.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ Final feature matrix exported to '{output_filename}'\")\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"  ‚Ä¢ Total senders: {len(final_features):,}\")\n",
    "    print(f\"  ‚Ä¢ Features selected: {len(top_features)}\")\n",
    "    print(f\"  ‚Ä¢ Phishing senders: {(final_features['label']==1).sum():,} ({(final_features['label']==1).mean()*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Legitimate senders: {(final_features['label']==0).sum():,} ({(final_features['label']==0).mean()*100:.1f}%)\")\n",
    "\n",
    "    print(f\"\\nüìã Top features included:\")\n",
    "    for i in range(0, len(top_features), 3):\n",
    "        print(\"  \", \", \".join(top_features[i:i+3]))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüéØ Ready for model training with temporal features!\")\n",
    "    print(f\"   Output: {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA4263_Group_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
