{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Email Data Cleaning Pipeline - Timezone-Aware Version\n",
    "\n",
    "This notebook performs comprehensive data cleaning with:\n",
    "1. Nanosecond precision handling in dates\n",
    "2. **Timezone preservation and extraction**\n",
    "3. **Timezone region classification**\n",
    "4. Data quality validation\n",
    "5. **Timezone-based grouping and analysis**\n",
    "\n",
    "**Goal:** Preserve maximum data including timezone context for regional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING RAW DATA\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset loaded:\n",
      "  ‚Ä¢ Rows: 49,301\n",
      "  ‚Ä¢ Columns: 7\n",
      "  ‚Ä¢ Column names: ['sender', 'receiver', 'date', 'subject', 'body', 'label', 'urls']\n",
      "\n",
      "üîç Checking date column format...\n",
      "  Sample date: Tue, 05 Aug 2008 16:31:02 -0700\n",
      "  ‚ö†Ô∏è Detected datetime with nanosecond precision or ISO format\n",
      "  ‚úÖ Will apply special handling...\n",
      "\n",
      "üìã Column data types:\n",
      "  ‚Ä¢ sender: object (nulls: 0)\n",
      "  ‚Ä¢ receiver: object (nulls: 0)\n",
      "  ‚Ä¢ date: object (nulls: 0)\n",
      "  ‚Ä¢ subject: object (nulls: 77)\n",
      "  ‚Ä¢ body: object (nulls: 1)\n",
      "  ‚Ä¢ label: int64 (nulls: 0)\n",
      "  ‚Ä¢ urls: int64 (nulls: 0)\n"
     ]
    }
   ],
   "source": [
    "# Load raw data with proper handling\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING RAW DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Try to load the data\n",
    "file_path = \"../data/processed/date_merge.csv\"\n",
    "df_raw = pd.read_csv(file_path)\n",
    "original_count = len(df_raw)\n",
    "\n",
    "print(f\"\\nüìä Dataset loaded:\")\n",
    "print(f\"  ‚Ä¢ Rows: {original_count:,}\")\n",
    "print(f\"  ‚Ä¢ Columns: {len(df_raw.columns)}\")\n",
    "print(f\"  ‚Ä¢ Column names: {df_raw.columns.tolist()}\")\n",
    "\n",
    "# Check if date column needs special handling\n",
    "print(\"\\nüîç Checking date column format...\")\n",
    "sample_date = str(df_raw['date'].iloc[0])\n",
    "print(f\"  Sample date: {sample_date}\")\n",
    "\n",
    "# Check for nanosecond precision issue\n",
    "if '.000000000' in sample_date or 'T' in sample_date:\n",
    "    print(\"  ‚ö†Ô∏è Detected datetime with nanosecond precision or ISO format\")\n",
    "    print(\"  ‚úÖ Will apply special handling...\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nüìã Column data types:\")\n",
    "for col, dtype in df_raw.dtypes.items():\n",
    "    null_count = df_raw[col].isna().sum()\n",
    "    print(f\"  ‚Ä¢ {col}: {dtype} (nulls: {null_count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "timezone_mapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TIMEZONE REGION MAPPING SETUP\n",
      "================================================================================\n",
      "\n",
      "üåç Sample timezone region mappings:\n",
      "  UTC-12.0 ‚Üí Americas\n",
      "  UTC-8.0 ‚Üí Americas\n",
      "  UTC-5.0 ‚Üí Americas\n",
      "  UTC-4.0 ‚Üí Europe/Africa\n",
      "  UTC-3.0 ‚Üí Europe/Africa\n",
      "  UTC+0.0 ‚Üí Europe/Africa\n",
      "  UTC+2.0 ‚Üí Europe/Africa\n",
      "  UTC+3.0 ‚Üí Middle East/South Asia\n",
      "  UTC+5.5 ‚Üí Middle East/South Asia\n",
      "  UTC+6.0 ‚Üí Middle East/South Asia\n",
      "  UTC+6.5 ‚Üí APAC\n",
      "  UTC+8.0 ‚Üí APAC\n",
      "  UTC+9.5 ‚Üí APAC\n",
      "  UTC+10.0 ‚Üí APAC\n",
      "  UTC+10.5 ‚Üí Oceania/Pacific\n",
      "  UTC+12.0 ‚Üí Oceania/Pacific\n",
      "  UTC+14.0 ‚Üí Oceania/Pacific\n",
      "\n",
      "‚úÖ Timezone mapping function ready\n",
      "  ‚Ä¢ Valid range: UTC-12:00 to UTC+14:00\n",
      "  ‚Ä¢ Data outside this range will be removed\n"
     ]
    }
   ],
   "source": [
    "# Define timezone to region mapping\n",
    "print(\"=\"*80)\n",
    "print(\"TIMEZONE REGION MAPPING SETUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_simple_region(offset_hours):\n",
    "    \"\"\"\n",
    "    Simplified 6-region model for cleaner visualization\n",
    "    Valid range: UTC-12:00 to UTC+14:00\n",
    "    \n",
    "    Boundary handling:\n",
    "    - UTC-4 belongs to Americas (e.g., Atlantic Time)\n",
    "    - UTC+2 belongs to Europe/Africa (e.g., South Africa)\n",
    "    - UTC+6 belongs to Middle East/South Asia (e.g., Bangladesh)\n",
    "    - UTC+10 belongs to APAC (e.g., Australian Eastern Time)\n",
    "    \"\"\"\n",
    "    if pd.isna(offset_hours):\n",
    "        return 'Unknown'\n",
    "    elif -12 <= offset_hours < -4:\n",
    "        return 'Americas'\n",
    "    elif -4 <= offset_hours <= 2:\n",
    "        return 'Europe/Africa'\n",
    "    elif 2 < offset_hours <= 6:\n",
    "        return 'Middle East/South Asia'  # Includes all .5 offsets (Iran, India, etc.)\n",
    "    elif 6 < offset_hours <= 10:\n",
    "        return 'APAC'  # Includes Myanmar (6.5), Australia Central (9.5)\n",
    "    elif 10 < offset_hours <= 14:\n",
    "        return 'Oceania/Pacific'  # Includes Australia (10.5)\n",
    "    else:\n",
    "        return 'Invalid'\n",
    "\n",
    "# Test the function with boundary values\n",
    "print(\"\\nüåç Sample timezone region mappings:\")\n",
    "test_offsets = [-12, -8, -5, -4, -3, 0, 2, 3, 5.5, 6, 6.5, 8, 9.5, 10, 10.5, 12, 14]\n",
    "for offset in test_offsets:\n",
    "    region = get_simple_region(offset)\n",
    "    print(f\"  UTC{offset:+.1f} ‚Üí {region}\")\n",
    "\n",
    "print(\"\\n‚úÖ Timezone mapping function ready\")\n",
    "print(\"  ‚Ä¢ Valid range: UTC-12:00 to UTC+14:00\")\n",
    "print(\"  ‚Ä¢ Data outside this range will be removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "parse_dates_with_timezone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATE PARSING WITH TIMEZONE PRESERVATION\n",
      "================================================================================\n",
      "\n",
      "üìÖ Step 1: Extracting timezone information...\n",
      "  ‚Ä¢ Dates with timezone: 48,242\n",
      "  ‚Ä¢ Dates without timezone: 1,059\n",
      "  ‚ÑπÔ∏è  Missing timezones will be treated as UTC (+0000)\n",
      "\n",
      "üîç Step 2: Validating timezone ranges...\n",
      "  ‚ö†Ô∏è  Found 388 emails with invalid timezones (outside UTC-12 to UTC+14)\n",
      "  üóëÔ∏è  Removing these 388 rows from the dataset\n",
      "  ‚úÖ Kept 48,913 rows with valid timezones\n",
      "\n",
      "üåç Step 3: Classifying timezone regions...\n",
      "\n",
      "  Timezone region distribution:\n",
      "    ‚Ä¢ Europe/Africa: 27,500 (56.2%)\n",
      "    ‚Ä¢ Americas: 12,778 (26.1%)\n",
      "    ‚Ä¢ Middle East/South Asia: 4,785 (9.8%)\n",
      "    ‚Ä¢ APAC: 2,857 (5.8%)\n",
      "    ‚Ä¢ Oceania/Pacific: 993 (2.0%)\n",
      "\n",
      "üìÖ Step 4: Parsing dates (keeping original timezone context)...\n",
      "  ‚Ä¢ Successfully parsed: 48,836\n",
      "  ‚Ä¢ Failed to parse: 77\n",
      "\n",
      "  ‚ö†Ô∏è Removing 77 rows with unparseable dates\n",
      "\n",
      "‚úÖ Date column type: datetime64[ns]\n",
      "\n",
      "‚úÖ Date parsing complete with timezone preservation!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Date Parsing with Timezone Preservation\n",
    "print(\"=\"*80)\n",
    "print(\"DATE PARSING WITH TIMEZONE PRESERVATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Extract timezone information\n",
    "print(\"\\nüìÖ Step 1: Extracting timezone information...\")\n",
    "\n",
    "# Extract timezone offset from date string\n",
    "df_raw['timezone_offset'] = df_raw['date'].astype(str).str.extract(r'([+-]\\d{4})$')[0]\n",
    "\n",
    "# Count timezones\n",
    "has_timezone = df_raw['timezone_offset'].notna().sum()\n",
    "no_timezone = df_raw['timezone_offset'].isna().sum()\n",
    "\n",
    "print(f\"  ‚Ä¢ Dates with timezone: {has_timezone:,}\")\n",
    "print(f\"  ‚Ä¢ Dates without timezone: {no_timezone:,}\")\n",
    "\n",
    "if no_timezone > 0:\n",
    "    print(f\"  ‚ÑπÔ∏è  Missing timezones will be treated as UTC (+0000)\")\n",
    "\n",
    "# Convert timezone offset to hours for easier handling\n",
    "def parse_timezone_offset(tz_str):\n",
    "    \"\"\"Convert timezone string like '+0800' or '-0700' to hours\"\"\"\n",
    "    if pd.isna(tz_str):\n",
    "        # Treat missing timezone as UTC (+0000)\n",
    "        return 0.0\n",
    "    try:\n",
    "        sign = 1 if tz_str[0] == '+' else -1\n",
    "        hours = int(tz_str[1:3])\n",
    "        minutes = int(tz_str[3:5])\n",
    "        return sign * (hours + minutes / 60)\n",
    "    except:\n",
    "        # If parsing fails, default to UTC\n",
    "        return 0.0\n",
    "\n",
    "df_raw['timezone_hours'] = df_raw['timezone_offset'].apply(parse_timezone_offset)\n",
    "\n",
    "# Remove rows with timezones outside valid range (UTC-12 to UTC+14)\n",
    "print(\"\\nüîç Step 2: Validating timezone ranges...\")\n",
    "valid_tz_mask = (df_raw['timezone_hours'] >= -12) & (df_raw['timezone_hours'] <= 14)\n",
    "invalid_tz_count = (~valid_tz_mask).sum()\n",
    "\n",
    "if invalid_tz_count > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  Found {invalid_tz_count} emails with invalid timezones (outside UTC-12 to UTC+14)\")\n",
    "    print(f\"  üóëÔ∏è  Removing these {invalid_tz_count} rows from the dataset\")\n",
    "    df_raw = df_raw[valid_tz_mask].copy()\n",
    "    print(f\"  ‚úÖ Kept {len(df_raw):,} rows with valid timezones\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ All timezones are valid (within UTC-12 to UTC+14)\")\n",
    "\n",
    "# Step 3: Add timezone region using simplified mapping\n",
    "print(\"\\nüåç Step 3: Classifying timezone regions...\")\n",
    "df_raw['timezone_region'] = df_raw['timezone_hours'].apply(get_simple_region)\n",
    "\n",
    "# Verify no 'Invalid' regions were created (safety check)\n",
    "invalid_regions = (df_raw['timezone_region'] == 'Invalid').sum()\n",
    "if invalid_regions > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  WARNING: Found {invalid_regions} emails classified as 'Invalid' region\")\n",
    "    print(f\"  üóëÔ∏è  Removing these rows\")\n",
    "    df_raw = df_raw[df_raw['timezone_region'] != 'Invalid'].copy()\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\n  Timezone region distribution:\")\n",
    "region_counts = df_raw['timezone_region'].value_counts()\n",
    "for region, count in region_counts.items():\n",
    "    pct = count / len(df_raw) * 100\n",
    "    print(f\"    ‚Ä¢ {region}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Step 4: Parse dates while preserving timezone\n",
    "print(\"\\nüìÖ Step 4: Parsing dates (keeping original timezone context)...\")\n",
    "\n",
    "def parse_email_date_preserve_tz(date_str):\n",
    "    \"\"\"\n",
    "    Parse email date string and return UTC datetime\n",
    "    (but we keep timezone info in separate columns)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try standard email format with day name\n",
    "        # Example: \"Tue, 05 Aug 2008 16:31:02 -0700\"\n",
    "        from email.utils import parsedate_to_datetime\n",
    "        dt = parsedate_to_datetime(date_str)\n",
    "        # Convert to UTC (timezone-naive) for consistent storage\n",
    "        return dt.replace(tzinfo=None) if dt.tzinfo is None else dt.astimezone(None).replace(tzinfo=None)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Fallback: try without day name\n",
    "        cleaned = re.sub(r'^\\w{3},\\s*', '', str(date_str))\n",
    "        dt = parsedate_to_datetime(cleaned)\n",
    "        return dt.replace(tzinfo=None) if dt.tzinfo is None else dt.astimezone(None).replace(tzinfo=None)\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "# Parse dates\n",
    "df_raw['date_utc'] = df_raw['date'].apply(parse_email_date_preserve_tz)\n",
    "\n",
    "# ‚úÖ FIX: Explicitly convert to datetime type\n",
    "df_raw['date_utc'] = pd.to_datetime(df_raw['date_utc'], errors='coerce')\n",
    "\n",
    "# Report parsing success\n",
    "parsed_count = df_raw['date_utc'].notna().sum()\n",
    "failed_count = df_raw['date_utc'].isna().sum()\n",
    "\n",
    "print(f\"  ‚Ä¢ Successfully parsed: {parsed_count:,}\")\n",
    "print(f\"  ‚Ä¢ Failed to parse: {failed_count:,}\")\n",
    "\n",
    "if failed_count > 0:\n",
    "    print(f\"\\n  ‚ö†Ô∏è Removing {failed_count:,} rows with unparseable dates\")\n",
    "    df_raw = df_raw[df_raw['date_utc'].notna()].copy()\n",
    "\n",
    "# Rename for clarity\n",
    "df_raw = df_raw.rename(columns={'date': 'date_original', 'date_utc': 'date'})\n",
    "\n",
    "# Verify the date column is now datetime type\n",
    "print(f\"\\n‚úÖ Date column type: {df_raw['date'].dtype}\")\n",
    "print(\"\\n‚úÖ Date parsing complete with timezone preservation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "validate_dates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATE VALIDATION AND ANOMALY DETECTION\n",
      "================================================================================\n",
      "\n",
      "üìÖ Valid date range: 1990-01-01 to 2025-12-31\n",
      "\n",
      "üîç Date anomalies:\n",
      "  ‚Ä¢ Dates before 1990-01-01: 18\n",
      "  ‚Ä¢ Dates after 2025-12-31: 9\n",
      "\n",
      "  ‚ö†Ô∏è Filtering out 27 anomalous dates\n",
      "\n",
      "üìä Final date statistics:\n",
      "  ‚Ä¢ Earliest date: 1990-01-01 17:57:46\n",
      "  ‚Ä¢ Latest date: 2022-12-27 10:56:49\n",
      "  ‚Ä¢ Date span: 12,047 days\n",
      "  ‚Ä¢ Unique dates: 45,189\n",
      "\n",
      "‚úÖ Date validation complete\n"
     ]
    }
   ],
   "source": [
    "# Validate and Clean Date Anomalies\n",
    "print(\"=\"*80)\n",
    "print(\"DATE VALIDATION AND ANOMALY DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define reasonable date range\n",
    "min_valid_date = pd.Timestamp('1990-01-01')\n",
    "max_valid_date = pd.Timestamp('2025-12-31')\n",
    "\n",
    "print(f\"\\nüìÖ Valid date range: {min_valid_date.date()} to {max_valid_date.date()}\")\n",
    "\n",
    "# Check for dates outside valid range\n",
    "too_old = (df_raw['date'] < min_valid_date).sum()\n",
    "too_new = (df_raw['date'] > max_valid_date).sum()\n",
    "\n",
    "print(f\"\\nüîç Date anomalies:\")\n",
    "print(f\"  ‚Ä¢ Dates before {min_valid_date.date()}: {too_old:,}\")\n",
    "print(f\"  ‚Ä¢ Dates after {max_valid_date.date()}: {too_new:,}\")\n",
    "\n",
    "if too_old > 0 or too_new > 0:\n",
    "    print(f\"\\n  ‚ö†Ô∏è Filtering out {too_old + too_new:,} anomalous dates\")\n",
    "    df_raw = df_raw[\n",
    "        (df_raw['date'] >= min_valid_date) & \n",
    "        (df_raw['date'] <= max_valid_date)\n",
    "    ].copy()\n",
    "\n",
    "# Show final date statistics\n",
    "print(f\"\\nüìä Final date statistics:\")\n",
    "print(f\"  ‚Ä¢ Earliest date: {df_raw['date'].min()}\")\n",
    "print(f\"  ‚Ä¢ Latest date: {df_raw['date'].max()}\")\n",
    "print(f\"  ‚Ä¢ Date span: {(df_raw['date'].max() - df_raw['date'].min()).days:,} days\")\n",
    "print(f\"  ‚Ä¢ Unique dates: {df_raw['date'].nunique():,}\")\n",
    "\n",
    "print(\"\\n‚úÖ Date validation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "extract_temporal_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXTRACTING TEMPORAL FEATURES\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Extracted temporal features:\n",
      "  ‚Ä¢ year, month, day, hour\n",
      "  ‚Ä¢ day_of_week, day_name, is_weekend\n",
      "\n",
      "üìã Sample of temporal features:\n",
      "                 date  year  month  hour   day_name  is_weekend  \\\n",
      "0 2008-08-06 07:31:02  2008      8     7  Wednesday           0   \n",
      "1 2008-08-06 07:31:03  2008      8     7  Wednesday           0   \n",
      "2 2008-08-06 16:28:00  2008      8    16  Wednesday           0   \n",
      "3 2008-08-06 07:31:20  2008      8     7  Wednesday           0   \n",
      "4 2008-08-06 07:31:21  2008      8     7  Wednesday           0   \n",
      "5 2008-08-06 07:31:22  2008      8     7  Wednesday           0   \n",
      "6 2008-08-06 05:44:01  2008      8     5  Wednesday           0   \n",
      "7 2008-08-06 07:41:14  2008      8     7  Wednesday           0   \n",
      "8 2008-08-06 07:31:03  2008      8     7  Wednesday           0   \n",
      "9 2008-08-06 07:31:38  2008      8     7  Wednesday           0   \n",
      "\n",
      "   timezone_hours         timezone_region  \n",
      "0           -7.00                Americas  \n",
      "1           -5.00                Americas  \n",
      "2          -12.00                Americas  \n",
      "3           -6.00                Americas  \n",
      "4           -4.00           Europe/Africa  \n",
      "5            6.00  Middle East/South Asia  \n",
      "6            0.00           Europe/Africa  \n",
      "7           -3.00           Europe/Africa  \n",
      "8           -8.00                Americas  \n",
      "9            1.00           Europe/Africa  \n"
     ]
    }
   ],
   "source": [
    "# Extract Temporal Features for Analysis\n",
    "print(\"=\"*80)\n",
    "print(\"EXTRACTING TEMPORAL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract useful temporal components\n",
    "df_raw['year'] = df_raw['date'].dt.year\n",
    "df_raw['month'] = df_raw['date'].dt.month\n",
    "df_raw['day'] = df_raw['date'].dt.day\n",
    "df_raw['hour'] = df_raw['date'].dt.hour\n",
    "df_raw['day_of_week'] = df_raw['date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df_raw['day_name'] = df_raw['date'].dt.day_name()\n",
    "df_raw['is_weekend'] = df_raw['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"\\n‚úÖ Extracted temporal features:\")\n",
    "print(\"  ‚Ä¢ year, month, day, hour\")\n",
    "print(\"  ‚Ä¢ day_of_week, day_name, is_weekend\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìã Sample of temporal features:\")\n",
    "print(df_raw[['date', 'year', 'month', 'hour', 'day_name', 'is_weekend', 'timezone_hours', 'timezone_region']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "final_quality_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL DATA QUALITY CHECK\n",
      "================================================================================\n",
      "\n",
      "üìä Missing Values:\n",
      "  ‚Ä¢ subject: 77 (0.2%)\n",
      "  ‚Ä¢ body: 1 (0.0%)\n",
      "  ‚Ä¢ timezone_offset: 1,038 (2.1%)\n",
      "\n",
      "üìä Duplicate rows: 6\n",
      "  Removing duplicates...\n",
      "  ‚úÖ Removed 6 duplicate rows\n",
      "\n",
      "üìä Email Format Validation:\n",
      "  Senders with email format: 48,786 (100.0%)\n",
      "\n",
      "üìä Final Data Types:\n",
      "  ‚Ä¢ sender: object\n",
      "  ‚Ä¢ receiver: object\n",
      "  ‚Ä¢ date: datetime64[ns]\n",
      "  ‚Ä¢ subject: object\n",
      "  ‚Ä¢ label: int64\n",
      "  ‚Ä¢ timezone_hours: float64\n",
      "  ‚Ä¢ timezone_region: object\n",
      "\n",
      "üìä Final Dataset Size:\n",
      "  ‚Ä¢ Rows: 48,803\n",
      "  ‚Ä¢ Columns: 18\n",
      "  ‚Ä¢ Date range: 1990-01-01 17:57:46 to 2022-12-27 10:56:49\n"
     ]
    }
   ],
   "source": [
    "# Final Data Quality Check\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL DATA QUALITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nüìä Missing Values:\")\n",
    "for col in df_raw.columns:\n",
    "    missing = df_raw[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        missing_pct = missing / len(df_raw) * 100\n",
    "        print(f\"  ‚Ä¢ {col}: {missing:,} ({missing_pct:.1f}%)\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_raw.duplicated(subset=['sender', 'receiver', 'date', 'subject']).sum()\n",
    "print(f\"\\nüìä Duplicate rows: {duplicates:,}\")\n",
    "if duplicates > 0:\n",
    "    print(\"  Removing duplicates...\")\n",
    "    df_raw = df_raw.drop_duplicates(subset=['sender', 'receiver', 'date', 'subject']).copy()\n",
    "    print(f\"  ‚úÖ Removed {duplicates:,} duplicate rows\")\n",
    "\n",
    "# Validate email formats\n",
    "print(\"\\nüìä Email Format Validation:\")\n",
    "has_email_format = df_raw['sender'].str.contains(r'[<>@]', na=False).sum()\n",
    "print(f\"  Senders with email format: {has_email_format:,} ({has_email_format/len(df_raw)*100:.1f}%)\")\n",
    "\n",
    "# Final data types check\n",
    "print(\"\\nüìä Final Data Types:\")\n",
    "key_columns = ['sender', 'receiver', 'date', 'subject', 'label', 'timezone_hours', 'timezone_region']\n",
    "for col in key_columns:\n",
    "    if col in df_raw.columns:\n",
    "        print(f\"  ‚Ä¢ {col}: {df_raw[col].dtype}\")\n",
    "\n",
    "print(f\"\\nüìä Final Dataset Size:\")\n",
    "print(f\"  ‚Ä¢ Rows: {len(df_raw):,}\")\n",
    "print(f\"  ‚Ä¢ Columns: {len(df_raw.columns)}\")\n",
    "print(f\"  ‚Ä¢ Date range: {df_raw['date'].min()} to {df_raw['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "timezone_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TIMEZONE-BASED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä 1. EMAIL DISTRIBUTION BY TIMEZONE REGION\n",
      "================================================================================\n",
      "\n",
      "                        Total_Emails  Phishing_Count  Phishing_Rate  Legitimate_Count\n",
      "timezone_region                                                                      \n",
      "Europe/Africa                  27446           16379           0.60             11067\n",
      "Americas                       12751            4328           0.34              8423\n",
      "Middle East/South Asia          4776            4453           0.93               323\n",
      "APAC                            2844            2175           0.77               669\n",
      "Oceania/Pacific                  986              71           0.07               915\n",
      "\n",
      "\n",
      "üìä 2. PHISHING VS LEGITIMATE BY REGION\n",
      "================================================================================\n",
      "\n",
      "Europe/Africa:\n",
      "  Total: 27,446 emails\n",
      "  Phishing: 16,379 (59.7%)\n",
      "  Legitimate: 11,067 (40.3%)\n",
      "\n",
      "Americas:\n",
      "  Total: 12,751 emails\n",
      "  Phishing: 4,328 (33.9%)\n",
      "  Legitimate: 8,423 (66.1%)\n",
      "\n",
      "Middle East/South Asia:\n",
      "  Total: 4,776 emails\n",
      "  Phishing: 4,453 (93.2%)\n",
      "  Legitimate: 323 (6.8%)\n",
      "\n",
      "APAC:\n",
      "  Total: 2,844 emails\n",
      "  Phishing: 2,175 (76.5%)\n",
      "  Legitimate: 669 (23.5%)\n",
      "\n",
      "Oceania/Pacific:\n",
      "  Total: 986 emails\n",
      "  Phishing: 71 (7.2%)\n",
      "  Legitimate: 915 (92.8%)\n",
      "\n",
      "\n",
      "üìä 3. TOP 10 TIMEZONE HOURS (NUMERIC OFFSET)\n",
      "================================================================================\n",
      "\n",
      "                Total_Emails  Phishing_Count  Phishing_Rate\n",
      "timezone_hours                                             \n",
      "0.00                    8669            5009          57.80\n",
      "1.00                    5833            1947          33.40\n",
      "-4.00                   5017            3275          65.30\n",
      "-5.00                   5003            1954          39.10\n",
      "2.00                    4548            2995          65.90\n",
      "-7.00                   3966            1165          29.40\n",
      "-8.00                   2435             590          24.20\n",
      "3.00                    2050            1914          93.40\n",
      "-3.00                   1792            1606          89.60\n",
      "8.00                    1155             951          82.30\n",
      "\n",
      "\n",
      "üìä 4. TEMPORAL PATTERNS BY REGION\n",
      "================================================================================\n",
      "\n",
      "Most active hours by region (Top 3):\n",
      "\n",
      "Europe/Africa:\n",
      "  08:00 - 1,884 emails\n",
      "  21:00 - 1,440 emails\n",
      "  09:00 - 1,403 emails\n",
      "\n",
      "Americas:\n",
      "  08:00 - 957 emails\n",
      "  09:00 - 801 emails\n",
      "  10:00 - 749 emails\n",
      "\n",
      "Middle East/South Asia:\n",
      "  21:00 - 316 emails\n",
      "  20:00 - 315 emails\n",
      "  17:00 - 308 emails\n",
      "\n",
      "APAC:\n",
      "  10:00 - 204 emails\n",
      "  17:00 - 194 emails\n",
      "  12:00 - 178 emails\n",
      "\n",
      "Oceania/Pacific:\n",
      "  08:00 - 83 emails\n",
      "  11:00 - 69 emails\n",
      "  09:00 - 68 emails\n",
      "\n",
      "\n",
      "üìä 5. YEARLY TRENDS BY REGION (2008-2022)\n",
      "================================================================================\n",
      "\n",
      "Emails per year (top 5 years):\n",
      "\n",
      "2002: 5,416 total emails\n",
      "  APAC: 184 (3.4%)\n",
      "  Americas: 1,369 (25.3%)\n",
      "  Europe/Africa: 3,742 (69.1%)\n",
      "  Middle East/South Asia: 95 (1.8%)\n",
      "  Oceania/Pacific: 26 (0.5%)\n",
      "\n",
      "2005: 541 total emails\n",
      "  APAC: 18 (3.3%)\n",
      "  Americas: 82 (15.2%)\n",
      "  Europe/Africa: 396 (73.2%)\n",
      "  Middle East/South Asia: 42 (7.8%)\n",
      "  Oceania/Pacific: 3 (0.6%)\n",
      "\n",
      "2006: 990 total emails\n",
      "  APAC: 103 (10.4%)\n",
      "  Americas: 122 (12.3%)\n",
      "  Europe/Africa: 718 (72.5%)\n",
      "  Middle East/South Asia: 46 (4.6%)\n",
      "  Oceania/Pacific: 1 (0.1%)\n",
      "\n",
      "2007: 456 total emails\n",
      "  APAC: 34 (7.5%)\n",
      "  Americas: 94 (20.6%)\n",
      "  Europe/Africa: 318 (69.7%)\n",
      "  Middle East/South Asia: 9 (2.0%)\n",
      "  Oceania/Pacific: 1 (0.2%)\n",
      "\n",
      "2008: 38,838 total emails\n",
      "  APAC: 2,400 (6.2%)\n",
      "  Americas: 10,289 (26.5%)\n",
      "  Europe/Africa: 20,736 (53.4%)\n",
      "  Middle East/South Asia: 4,465 (11.5%)\n",
      "  Oceania/Pacific: 948 (2.4%)\n"
     ]
    }
   ],
   "source": [
    "# Timezone-Based Analysis\n",
    "print(\"=\"*80)\n",
    "print(\"TIMEZONE-BASED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Overall distribution by timezone region\n",
    "print(\"\\nüìä 1. EMAIL DISTRIBUTION BY TIMEZONE REGION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "region_stats = df_raw.groupby('timezone_region').agg({\n",
    "    'label': ['count', 'sum', 'mean']\n",
    "}).round(3)\n",
    "\n",
    "region_stats.columns = ['Total_Emails', 'Phishing_Count', 'Phishing_Rate']\n",
    "region_stats['Legitimate_Count'] = region_stats['Total_Emails'] - region_stats['Phishing_Count']\n",
    "region_stats = region_stats.sort_values('Total_Emails', ascending=False)\n",
    "\n",
    "print(\"\\n\" + region_stats.to_string())\n",
    "\n",
    "# 2. Phishing vs Legitimate by timezone region\n",
    "print(\"\\n\\nüìä 2. PHISHING VS LEGITIMATE BY REGION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for region in region_stats.index:\n",
    "    region_data = region_stats.loc[region]\n",
    "    total = region_data['Total_Emails']\n",
    "    phishing = region_data['Phishing_Count']\n",
    "    legitimate = region_data['Legitimate_Count']\n",
    "    phishing_pct = region_data['Phishing_Rate'] * 100\n",
    "    \n",
    "    print(f\"\\n{region}:\")\n",
    "    print(f\"  Total: {int(total):,} emails\")\n",
    "    print(f\"  Phishing: {int(phishing):,} ({phishing_pct:.1f}%)\")\n",
    "    print(f\"  Legitimate: {int(legitimate):,} ({100-phishing_pct:.1f}%)\")\n",
    "\n",
    "# 3. Top timezone hours (numeric offsets)\n",
    "print(\"\\n\\nüìä 3. TOP 10 TIMEZONE HOURS (NUMERIC OFFSET)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tz_hours_stats = df_raw.groupby('timezone_hours').agg({\n",
    "    'label': ['count', 'sum']\n",
    "}).round(0)\n",
    "\n",
    "tz_hours_stats.columns = ['Total_Emails', 'Phishing_Count']\n",
    "tz_hours_stats['Phishing_Rate'] = (tz_hours_stats['Phishing_Count'] / tz_hours_stats['Total_Emails'] * 100).round(1)\n",
    "tz_hours_stats = tz_hours_stats.sort_values('Total_Emails', ascending=False).head(10)\n",
    "\n",
    "print(\"\\n\" + tz_hours_stats.to_string())\n",
    "\n",
    "# 4. Temporal patterns by timezone region\n",
    "print(\"\\n\\nüìä 4. TEMPORAL PATTERNS BY REGION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Hour of day distribution by region\n",
    "print(\"\\nMost active hours by region (Top 3):\")\n",
    "for region in df_raw['timezone_region'].value_counts().head(5).index:\n",
    "    if region not in ['Unknown', 'Invalid']:\n",
    "        region_data = df_raw[df_raw['timezone_region'] == region]\n",
    "        top_hours = region_data['hour'].value_counts().head(3)\n",
    "        print(f\"\\n{region}:\")\n",
    "        for hour, count in top_hours.items():\n",
    "            print(f\"  {hour:02d}:00 - {count:,} emails\")\n",
    "\n",
    "# 5. Yearly trends by timezone region\n",
    "print(\"\\n\\nüìä 5. YEARLY TRENDS BY REGION (2008-2022)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "yearly_region = df_raw.groupby(['year', 'timezone_region']).size().unstack(fill_value=0)\n",
    "\n",
    "# Show years with most emails\n",
    "top_years = df_raw['year'].value_counts().head(5).index\n",
    "print(\"\\nEmails per year (top 5 years):\")\n",
    "for year in sorted(top_years):\n",
    "    if year in yearly_region.index:\n",
    "        year_total = yearly_region.loc[year].sum()\n",
    "        print(f\"\\n{year}: {int(year_total):,} total emails\")\n",
    "        for region in yearly_region.columns:\n",
    "            count = yearly_region.loc[year, region]\n",
    "            if count > 0:\n",
    "                pct = count / year_total * 100\n",
    "                print(f\"  {region}: {int(count):,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "584c5zruw62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EMAIL DISTRIBUTION BY TIMEZONE HOURS\n",
      "================================================================================\n",
      "\n",
      "üìä Total unique timezone offsets: 40\n",
      "\n",
      "Timezone     Total      Phishing   Legitimate   Phishing %  \n",
      "================================================================================\n",
      "UTC-12.0     44         44         0            100.0       %\n",
      "UTC-11.0     11         11         0            100.0       %\n",
      "UTC-10.0     74         68         6            91.9        %\n",
      "UTC-9.5      4          4          0            100.0       %\n",
      "UTC-9.0      55         53         2            96.4        %\n",
      "UTC-8.0      2,435      590        1,845        24.2        %\n",
      "UTC-7.0      3,966      1,165      2,801        29.4        %\n",
      "UTC-6.5      1          1          0            100.0       %\n",
      "UTC-6.0      1,133      413        720          36.5        %\n",
      "UTC-5.5      4          4          0            100.0       %\n",
      "UTC-5.0      5,003      1,954      3,049        39.1        %\n",
      "UTC-4.5      21         21         0            100.0       %\n",
      "UTC-4.0      5,017      3,275      1,742        65.3        %\n",
      "UTC-3.5      1          1          0            100.0       %\n",
      "UTC-3.0      1,792      1,606      186          89.6        %\n",
      "UTC-2.5      1          1          0            100.0       %\n",
      "UTC-2.0      680        641        39           94.3        %\n",
      "UTC-1.0      904        904        0            100.0       %\n",
      "UTC+0.0      8,669      5,009      3,660        57.8        %\n",
      "UTC+0.1      1          0          1            0.0         %\n",
      "UTC+1.0      5,833      1,947      3,886        33.4        %\n",
      "UTC+2.0      4,548      2,995      1,553        65.9        %\n",
      "UTC+3.0      2,050      1,914      136          93.4        %\n",
      "UTC+3.5      6          2          4            33.3        %\n",
      "UTC+4.0      672        635        37           94.5        %\n",
      "UTC+4.5      17         17         0            100.0       %\n",
      "UTC+5.0      1,115      1,109      6            99.5        %\n",
      "UTC+5.5      452        313        139          69.2        %\n",
      "UTC+5.8      1          1          0            100.0       %\n",
      "UTC+6.0      463        462        1            99.8        %\n",
      "UTC+6.5      1          1          0            100.0       %\n",
      "UTC+7.0      637        602        35           94.5        %\n",
      "UTC+8.0      1,155      951        204          82.3        %\n",
      "UTC+9.0      625        497        128          79.5        %\n",
      "UTC+9.5      6          3          3            50.0        %\n",
      "UTC+10.0     420        121        299          28.8        %\n",
      "UTC+10.5     5          0          5            0.0         %\n",
      "UTC+11.0     172        18         154          10.5        %\n",
      "UTC+12.0     386        48         338          12.4        %\n",
      "UTC+13.0     423        5          418          1.2         %\n",
      "\n",
      "\n",
      "üìä TOP 10 TIMEZONE HOURS BY EMAIL VOLUME\n",
      "================================================================================\n",
      "\n",
      "Rank   Timezone     Total      Phishing   Legitimate   Phishing %  \n",
      "================================================================================\n",
      "1      UTC+0.0      8,669      5,009      3,660        57.8        %\n",
      "2      UTC+1.0      5,833      1,947      3,886        33.4        %\n",
      "3      UTC-4.0      5,017      3,275      1,742        65.3        %\n",
      "4      UTC-5.0      5,003      1,954      3,049        39.1        %\n",
      "5      UTC+2.0      4,548      2,995      1,553        65.9        %\n",
      "6      UTC-7.0      3,966      1,165      2,801        29.4        %\n",
      "7      UTC-8.0      2,435      590        1,845        24.2        %\n",
      "8      UTC+3.0      2,050      1,914      136          93.4        %\n",
      "9      UTC-3.0      1,792      1,606      186          89.6        %\n",
      "10     UTC+8.0      1,155      951        204          82.3        %\n",
      "\n",
      "\n",
      "üìä TIMEZONE HOURS WITH HIGHEST PHISHING RATES (min 100 emails)\n",
      "================================================================================\n",
      "\n",
      "Rank   Timezone     Total      Phishing   Phishing %  \n",
      "================================================================================\n",
      "1      UTC-1.0      904        904        100.0       %\n",
      "2      UTC+6.0      463        462        99.8        %\n",
      "3      UTC+5.0      1,115      1,109      99.5        %\n",
      "4      UTC+4.0      672        635        94.5        %\n",
      "5      UTC+7.0      637        602        94.5        %\n",
      "6      UTC-2.0      680        641        94.3        %\n",
      "7      UTC+3.0      2,050      1,914      93.4        %\n",
      "8      UTC-3.0      1,792      1,606      89.6        %\n",
      "9      UTC+8.0      1,155      951        82.3        %\n",
      "10     UTC+9.0      625        497        79.5        %\n",
      "\n",
      "\n",
      "üìä TIMEZONE HOURS WITH LOWEST PHISHING RATES (min 100 emails)\n",
      "================================================================================\n",
      "\n",
      "Rank   Timezone     Total      Legitimate   Phishing %  \n",
      "================================================================================\n",
      "1      UTC+13.0     423        418          1.2         %\n",
      "2      UTC+11.0     172        154          10.5        %\n",
      "3      UTC+12.0     386        338          12.4        %\n",
      "4      UTC-8.0      2,435      1,845        24.2        %\n",
      "5      UTC+10.0     420        299          28.8        %\n",
      "6      UTC-7.0      3,966      2,801        29.4        %\n",
      "7      UTC+1.0      5,833      3,886        33.4        %\n",
      "8      UTC-6.0      1,133      720          36.5        %\n",
      "9      UTC-5.0      5,003      3,049        39.1        %\n",
      "10     UTC+0.0      8,669      3,660        57.8        %\n",
      "\n",
      "‚úÖ Timezone hours distribution analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Email Distribution by Timezone Hours (Detailed)\n",
    "print(\"=\"*80)\n",
    "print(\"EMAIL DISTRIBUTION BY TIMEZONE HOURS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Group by timezone_hours and calculate statistics\n",
    "tz_hours_distribution = df_raw.groupby('timezone_hours').agg({\n",
    "    'label': ['count', 'sum', 'mean']\n",
    "}).round(3)\n",
    "\n",
    "tz_hours_distribution.columns = ['Total_Emails', 'Phishing_Count', 'Phishing_Rate']\n",
    "tz_hours_distribution['Legitimate_Count'] = tz_hours_distribution['Total_Emails'] - tz_hours_distribution['Phishing_Count']\n",
    "tz_hours_distribution['Phishing_Percentage'] = (tz_hours_distribution['Phishing_Rate'] * 100).round(1)\n",
    "\n",
    "# Sort by timezone hours (ascending)\n",
    "tz_hours_distribution = tz_hours_distribution.sort_index()\n",
    "\n",
    "print(f\"\\nüìä Total unique timezone offsets: {len(tz_hours_distribution)}\")\n",
    "print(f\"\\n{'Timezone':<12} {'Total':<10} {'Phishing':<10} {'Legitimate':<12} {'Phishing %':<12}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for tz_hour, row in tz_hours_distribution.iterrows():\n",
    "    if pd.isna(tz_hour):\n",
    "        tz_label = \"Unknown\"\n",
    "    else:\n",
    "        tz_label = f\"UTC{tz_hour:+.1f}\"\n",
    "    \n",
    "    total = int(row['Total_Emails'])\n",
    "    phishing = int(row['Phishing_Count'])\n",
    "    legitimate = int(row['Legitimate_Count'])\n",
    "    phishing_pct = row['Phishing_Percentage']\n",
    "    \n",
    "    print(f\"{tz_label:<12} {total:<10,} {phishing:<10,} {legitimate:<12,} {phishing_pct:<12.1f}%\")\n",
    "\n",
    "# Show top 10 by volume\n",
    "print(\"\\n\\nüìä TOP 10 TIMEZONE HOURS BY EMAIL VOLUME\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_10_tz = tz_hours_distribution.sort_values('Total_Emails', ascending=False).head(10)\n",
    "\n",
    "print(f\"\\n{'Rank':<6} {'Timezone':<12} {'Total':<10} {'Phishing':<10} {'Legitimate':<12} {'Phishing %':<12}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for rank, (tz_hour, row) in enumerate(top_10_tz.iterrows(), 1):\n",
    "    if pd.isna(tz_hour):\n",
    "        tz_label = \"Unknown\"\n",
    "    else:\n",
    "        tz_label = f\"UTC{tz_hour:+.1f}\"\n",
    "    \n",
    "    total = int(row['Total_Emails'])\n",
    "    phishing = int(row['Phishing_Count'])\n",
    "    legitimate = int(row['Legitimate_Count'])\n",
    "    phishing_pct = row['Phishing_Percentage']\n",
    "    \n",
    "    print(f\"{rank:<6} {tz_label:<12} {total:<10,} {phishing:<10,} {legitimate:<12,} {phishing_pct:<12.1f}%\")\n",
    "\n",
    "# Show timezone hours with highest phishing rates (min 100 emails)\n",
    "print(\"\\n\\nüìä TIMEZONE HOURS WITH HIGHEST PHISHING RATES (min 100 emails)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "high_phishing_tz = tz_hours_distribution[tz_hours_distribution['Total_Emails'] >= 100].sort_values('Phishing_Rate', ascending=False).head(10)\n",
    "\n",
    "print(f\"\\n{'Rank':<6} {'Timezone':<12} {'Total':<10} {'Phishing':<10} {'Phishing %':<12}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for rank, (tz_hour, row) in enumerate(high_phishing_tz.iterrows(), 1):\n",
    "    if pd.isna(tz_hour):\n",
    "        tz_label = \"Unknown\"\n",
    "    else:\n",
    "        tz_label = f\"UTC{tz_hour:+.1f}\"\n",
    "    \n",
    "    total = int(row['Total_Emails'])\n",
    "    phishing = int(row['Phishing_Count'])\n",
    "    phishing_pct = row['Phishing_Percentage']\n",
    "    \n",
    "    print(f\"{rank:<6} {tz_label:<12} {total:<10,} {phishing:<10,} {phishing_pct:<12.1f}%\")\n",
    "\n",
    "# Show timezone hours with lowest phishing rates (min 100 emails)\n",
    "print(\"\\n\\nüìä TIMEZONE HOURS WITH LOWEST PHISHING RATES (min 100 emails)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "low_phishing_tz = tz_hours_distribution[tz_hours_distribution['Total_Emails'] >= 100].sort_values('Phishing_Rate', ascending=True).head(10)\n",
    "\n",
    "print(f\"\\n{'Rank':<6} {'Timezone':<12} {'Total':<10} {'Legitimate':<12} {'Phishing %':<12}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for rank, (tz_hour, row) in enumerate(low_phishing_tz.iterrows(), 1):\n",
    "    if pd.isna(tz_hour):\n",
    "        tz_label = \"Unknown\"\n",
    "    else:\n",
    "        tz_label = f\"UTC{tz_hour:+.1f}\"\n",
    "    \n",
    "    total = int(row['Total_Emails'])\n",
    "    legitimate = int(row['Legitimate_Count'])\n",
    "    phishing_pct = row['Phishing_Percentage']\n",
    "    \n",
    "    print(f\"{rank:<6} {tz_label:<12} {total:<10,} {legitimate:<12,} {phishing_pct:<12.1f}%\")\n",
    "\n",
    "print(\"\\n‚úÖ Timezone hours distribution analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "weekend_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WEEKEND VS WEEKDAY ANALYSIS BY TIMEZONE\n",
      "================================================================================\n",
      "\n",
      "Emails sent during weekends vs weekdays by region:\n",
      "\n",
      "(0 = Weekday, 1 = Weekend)\n",
      "\n",
      "\n",
      "Europe/Africa:\n",
      "  Weekday: 26,071 emails, 15,468 phishing (59.3%)\n",
      "  Weekend: 1,375 emails, 911 phishing (66.3%)\n",
      "\n",
      "Americas:\n",
      "  Weekday: 12,176 emails, 3,943 phishing (32.4%)\n",
      "  Weekend: 575 emails, 385 phishing (67.0%)\n",
      "\n",
      "Middle East/South Asia:\n",
      "  Weekday: 4,702 emails, 4,386 phishing (93.3%)\n",
      "  Weekend: 74 emails, 67 phishing (90.5%)\n",
      "\n",
      "APAC:\n",
      "  Weekday: 2,736 emails, 2,081 phishing (76.1%)\n",
      "  Weekend: 108 emails, 94 phishing (87.0%)\n",
      "\n",
      "Oceania/Pacific:\n",
      "  Weekday: 980 emails, 71 phishing (7.2%)\n",
      "  Weekend: 6 emails, 0 phishing (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Weekend vs Weekday Analysis by Timezone\n",
    "print(\"=\"*80)\n",
    "print(\"WEEKEND VS WEEKDAY ANALYSIS BY TIMEZONE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "weekend_analysis = df_raw.groupby(['timezone_region', 'is_weekend']).agg({\n",
    "    'label': ['count', 'sum', 'mean']\n",
    "}).round(3)\n",
    "\n",
    "weekend_analysis.columns = ['Total_Emails', 'Phishing_Count', 'Phishing_Rate']\n",
    "\n",
    "print(\"\\nEmails sent during weekends vs weekdays by region:\")\n",
    "print(\"\\n(0 = Weekday, 1 = Weekend)\\n\")\n",
    "\n",
    "for region in df_raw['timezone_region'].value_counts().head(5).index:\n",
    "    if region in weekend_analysis.index.get_level_values(0):\n",
    "        print(f\"\\n{region}:\")\n",
    "        region_data = weekend_analysis.loc[region]\n",
    "        \n",
    "        for is_weekend, row in region_data.iterrows():\n",
    "            day_type = \"Weekend\" if is_weekend == 1 else \"Weekday\"\n",
    "            total = row['Total_Emails']\n",
    "            phishing = row['Phishing_Count']\n",
    "            rate = row['Phishing_Rate'] * 100\n",
    "            \n",
    "            print(f\"  {day_type}: {int(total):,} emails, {int(phishing):,} phishing ({rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "save_clean_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING CLEANED DATA WITH TIMEZONE INFORMATION\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Data saved to: cleaned_date_merged_data.csv\n",
      "  ‚Ä¢ Rows: 48,803\n",
      "  ‚Ä¢ Columns: 16\n",
      "  ‚Ä¢ New columns added:\n",
      "    - timezone_hours (numeric offset in hours, e.g., 8.0, -7.0)\n",
      "    - timezone_region (geographic classification)\n",
      "    - year, month, day, hour, day_of_week, day_name, is_weekend\n",
      "\n",
      "üìä FINAL SUMMARY:\n",
      "================================================================================\n",
      "  Started with: 49,301 rows\n",
      "  Ended with: 48,803 rows\n",
      "  Data retention: 98.99%\n",
      "  Date range: 1990-01-01 17:57:46 to 2022-12-27 10:56:49\n",
      "  Time span: 12,047 days\n",
      "\n",
      "  Label distribution:\n",
      "    ‚Ä¢ Phishing: 27,406 (56.2%)\n",
      "    ‚Ä¢ Legitimate: 21,397 (43.8%)\n",
      "\n",
      "  Timezone regions:\n",
      "    ‚Ä¢ Europe/Africa: 27,446 (56.2%)\n",
      "    ‚Ä¢ Americas: 12,751 (26.1%)\n",
      "    ‚Ä¢ Middle East/South Asia: 4,776 (9.8%)\n",
      "    ‚Ä¢ APAC: 2,844 (5.8%)\n",
      "    ‚Ä¢ Oceania/Pacific: 986 (2.0%)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ DATA CLEANING COMPLETE WITH TIMEZONE ANALYSIS!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data with timezone information\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING CLEANED DATA WITH TIMEZONE INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare final dataset\n",
    "essential_cols = [\n",
    "    'sender', 'receiver', 'date', 'subject', 'body', 'label', 'urls',\n",
    "    'timezone_hours', 'timezone_region',\n",
    "    'year', 'month', 'day', 'hour', 'day_of_week', 'day_name', 'is_weekend'\n",
    "]\n",
    "\n",
    "# Keep only columns that exist\n",
    "final_cols = [col for col in essential_cols if col in df_raw.columns]\n",
    "df_final = df_raw[final_cols].copy()\n",
    "\n",
    "# Format date to avoid nanosecond issues when saving\n",
    "df_final['date'] = df_final['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"cleaned_date_merged_data.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Data saved to: {output_path}\")\n",
    "print(f\"  ‚Ä¢ Rows: {len(df_final):,}\")\n",
    "print(f\"  ‚Ä¢ Columns: {len(df_final.columns)}\")\n",
    "print(f\"  ‚Ä¢ New columns added:\")\n",
    "print(f\"    - timezone_hours (numeric offset in hours, e.g., 8.0, -7.0)\")\n",
    "print(f\"    - timezone_region (geographic classification)\")\n",
    "print(f\"    - year, month, day, hour, day_of_week, day_name, is_weekend\")\n",
    "\n",
    "# Convert date back to datetime for immediate use\n",
    "df_final['date'] = pd.to_datetime(df_final['date'])\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nüìä FINAL SUMMARY:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Started with: {original_count:,} rows\")\n",
    "print(f\"  Ended with: {len(df_final):,} rows\")\n",
    "print(f\"  Data retention: {len(df_final)/original_count*100:.2f}%\")\n",
    "print(f\"  Date range: {df_final['date'].min()} to {df_final['date'].max()}\")\n",
    "print(f\"  Time span: {(df_final['date'].max() - df_final['date'].min()).days:,} days\")\n",
    "\n",
    "# Label distribution\n",
    "phishing = (df_final['label'] == 1).sum()\n",
    "legitimate = (df_final['label'] == 0).sum()\n",
    "print(f\"\\n  Label distribution:\")\n",
    "print(f\"    ‚Ä¢ Phishing: {phishing:,} ({phishing/len(df_final)*100:.1f}%)\")\n",
    "print(f\"    ‚Ä¢ Legitimate: {legitimate:,} ({legitimate/len(df_final)*100:.1f}%)\")\n",
    "\n",
    "# Timezone distribution\n",
    "print(f\"\\n  Timezone regions:\")\n",
    "for region, count in df_final['timezone_region'].value_counts().head(5).items():\n",
    "    pct = count / len(df_final) * 100\n",
    "    print(f\"    ‚Ä¢ {region}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ DATA CLEANING COMPLETE WITH TIMEZONE ANALYSIS!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Return the cleaned dataframe for further use\n",
    "df_cleaned = df_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "loading_instructions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HOW TO LOAD THE CLEANED DATA IN OTHER NOTEBOOKS\n",
      "================================================================================\n",
      "\n",
      "To load this cleaned data with timezone information in your analysis notebooks, use:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Load the cleaned data\n",
      "df = pd.read_csv('cleaned_date_merged_data.csv')\n",
      "\n",
      "# Parse the date column\n",
      "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
      "\n",
      "# Verify\n",
      "print(f\"Loaded {len(df):,} rows\")\n",
      "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
      "print(f\"Timezone regions: {df['timezone_region'].unique()}\")\n",
      "\n",
      "# Example: Group by timezone region\n",
      "region_analysis = df.groupby('timezone_region')['label'].agg(['count', 'sum', 'mean'])\n",
      "print(\"\\nPhishing rates by region:\")\n",
      "print(region_analysis)\n",
      "\n",
      "# Example: Filter by specific region\n",
      "asia_emails = df[df['timezone_region'] == 'APAC']\n",
      "print(f\"\\nEmails from APAC: {len(asia_emails):,}\")\n",
      "\n",
      "# Example: Filter by timezone hours (numeric comparisons)\n",
      "east_asian_tz = df[(df['timezone_hours'] >= 7) & (df['timezone_hours'] <= 9)]\n",
      "print(f\"\\nEmails from UTC+7 to UTC+9: {len(east_asian_tz):,}\")\n",
      "\n",
      "# Example: Analyze by timezone hours\n",
      "tz_analysis = df.groupby('timezone_hours')['label'].agg(['count', 'mean'])\n",
      "print(\"\\nTop timezones by volume:\")\n",
      "print(tz_analysis.sort_values('count', ascending=False).head(10))\n",
      "```\n",
      "\n",
      "New columns available:\n",
      "- timezone_hours: Numeric timezone offset in hours (e.g., 8.0, -7.0) - useful for filtering and analysis\n",
      "- timezone_region: Geographic region classification (Americas, Europe/Africa, Middle East/South Asia, APAC, Oceania/Pacific)\n",
      "- year, month, day, hour: Temporal components extracted from date\n",
      "- day_of_week, day_name: Day information (0=Monday, 6=Sunday)\n",
      "- is_weekend: Binary indicator (1=weekend, 0=weekday)\n",
      "\n",
      "\n",
      "‚úÖ Your data is clean and ready for timezone-based analysis!\n"
     ]
    }
   ],
   "source": [
    "# Instructions for loading the cleaned data\n",
    "print(\"=\"*80)\n",
    "print(\"HOW TO LOAD THE CLEANED DATA IN OTHER NOTEBOOKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "To load this cleaned data with timezone information in your analysis notebooks, use:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned data\n",
    "df = pd.read_csv('cleaned_date_merged_data.csv')\n",
    "\n",
    "# Parse the date column\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Verify\n",
    "print(f\"Loaded {len(df):,} rows\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Timezone regions: {df['timezone_region'].unique()}\")\n",
    "\n",
    "# Example: Group by timezone region\n",
    "region_analysis = df.groupby('timezone_region')['label'].agg(['count', 'sum', 'mean'])\n",
    "print(\"\\\\nPhishing rates by region:\")\n",
    "print(region_analysis)\n",
    "\n",
    "# Example: Filter by specific region\n",
    "asia_emails = df[df['timezone_region'] == 'APAC']\n",
    "print(f\"\\\\nEmails from APAC: {len(asia_emails):,}\")\n",
    "\n",
    "# Example: Filter by timezone hours (numeric comparisons)\n",
    "east_asian_tz = df[(df['timezone_hours'] >= 7) & (df['timezone_hours'] <= 9)]\n",
    "print(f\"\\\\nEmails from UTC+7 to UTC+9: {len(east_asian_tz):,}\")\n",
    "\n",
    "# Example: Analyze by timezone hours\n",
    "tz_analysis = df.groupby('timezone_hours')['label'].agg(['count', 'mean'])\n",
    "print(\"\\\\nTop timezones by volume:\")\n",
    "print(tz_analysis.sort_values('count', ascending=False).head(10))\n",
    "```\n",
    "\n",
    "New columns available:\n",
    "- timezone_hours: Numeric timezone offset in hours (e.g., 8.0, -7.0) - useful for filtering and analysis\n",
    "- timezone_region: Geographic region classification (Americas, Europe/Africa, Middle East/South Asia, APAC, Oceania/Pacific)\n",
    "- year, month, day, hour: Temporal components extracted from date\n",
    "- day_of_week, day_name: Day information (0=Monday, 6=Sunday)\n",
    "- is_weekend: Binary indicator (1=weekend, 0=weekday)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ Your data is clean and ready for timezone-based analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "create_summary_report",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TIMEZONE ANALYSIS SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "TIMEZONE-AWARE EMAIL DATASET SUMMARY\n",
      "================================================================================\n",
      "\n",
      "DATASET OVERVIEW:\n",
      "- Total emails: 48,803\n",
      "- Date range: 1990-01-01 17:57:46 to 2022-12-27 10:56:49\n",
      "- Time span: 12,047 days\n",
      "- Phishing emails: 27,406 (56.2%)\n",
      "- Legitimate emails: 21,397 (43.8%)\n",
      "\n",
      "TIMEZONE COVERAGE:\n",
      "- Emails with timezone info: 48,803 (100.0%)\n",
      "- Unique timezone offsets: 40\n",
      "- Geographic regions covered: 5\n",
      "\n",
      "TOP TIMEZONE REGIONS:\n",
      "  Europe/Africa: 27,446 emails (56.2%) - Phishing rate: 59.7%\n",
      "  Americas: 12,751 emails (26.1%) - Phishing rate: 33.9%\n",
      "  Middle East/South Asia: 4,776 emails (9.8%) - Phishing rate: 93.2%\n",
      "  APAC: 2,844 emails (5.8%) - Phishing rate: 76.5%\n",
      "  Oceania/Pacific: 986 emails (2.0%) - Phishing rate: 7.2%\n",
      "\n",
      "TEMPORAL PATTERNS:\n",
      "- Weekend emails: 2,138 (4.4%)\n",
      "- Weekday emails: 46,665 (95.6%)\n",
      "- Most active year: 2008 (38,838 emails)\n",
      "- Peak hour: 08:00 (3,172 emails)\n",
      "\n",
      "DATA QUALITY:\n",
      "- Data retention rate: 98.99%\n",
      "- Missing timezone info: 0\n",
      "- Date parsing success: 100.00%\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Summary Report\n",
    "print(\"=\"*80)\n",
    "print(\"TIMEZONE ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "TIMEZONE-AWARE EMAIL DATASET SUMMARY\n",
    "{'='*80}\n",
    "\n",
    "DATASET OVERVIEW:\n",
    "- Total emails: {len(df_final):,}\n",
    "- Date range: {df_final['date'].min()} to {df_final['date'].max()}\n",
    "- Time span: {(df_final['date'].max() - df_final['date'].min()).days:,} days\n",
    "- Phishing emails: {(df_final['label'] == 1).sum():,} ({(df_final['label'] == 1).sum()/len(df_final)*100:.1f}%)\n",
    "- Legitimate emails: {(df_final['label'] == 0).sum():,} ({(df_final['label'] == 0).sum()/len(df_final)*100:.1f}%)\n",
    "\n",
    "TIMEZONE COVERAGE:\n",
    "- Emails with timezone info: {df_final['timezone_hours'].notna().sum():,} ({df_final['timezone_hours'].notna().sum()/len(df_final)*100:.1f}%)\n",
    "- Unique timezone offsets: {df_final['timezone_hours'].nunique()}\n",
    "- Geographic regions covered: {df_final['timezone_region'].nunique()}\n",
    "\n",
    "TOP TIMEZONE REGIONS:\n",
    "\"\"\"\n",
    "\n",
    "for region, count in df_final['timezone_region'].value_counts().head(5).items():\n",
    "    pct = count / len(df_final) * 100\n",
    "    phishing_count = df_final[df_final['timezone_region'] == region]['label'].sum()\n",
    "    phishing_rate = phishing_count / count * 100\n",
    "    summary_report += f\"  {region}: {count:,} emails ({pct:.1f}%) - Phishing rate: {phishing_rate:.1f}%\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "TEMPORAL PATTERNS:\n",
    "- Weekend emails: {(df_final['is_weekend'] == 1).sum():,} ({(df_final['is_weekend'] == 1).sum()/len(df_final)*100:.1f}%)\n",
    "- Weekday emails: {(df_final['is_weekend'] == 0).sum():,} ({(df_final['is_weekend'] == 0).sum()/len(df_final)*100:.1f}%)\n",
    "- Most active year: {df_final['year'].value_counts().index[0]} ({df_final['year'].value_counts().values[0]:,} emails)\n",
    "- Peak hour: {df_final['hour'].value_counts().index[0]:02d}:00 ({df_final['hour'].value_counts().values[0]:,} emails)\n",
    "\n",
    "DATA QUALITY:\n",
    "- Data retention rate: {len(df_final)/original_count*100:.2f}%\n",
    "- Missing timezone info: {df_final['timezone_hours'].isna().sum():,}\n",
    "- Date parsing success: {(df_final['date'].notna().sum() / len(df_final) * 100):.2f}%\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23243e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver</th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "      <th>timezone_hours</th>\n",
       "      <th>timezone_region</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_name</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Young Esposito &lt;Young@iworld.de&gt;</td>\n",
       "      <td>user4@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-08-06 07:31:02</td>\n",
       "      <td>Never agree to be a loser</td>\n",
       "      <td>Buck up, your troubles caused by small dimensi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.00</td>\n",
       "      <td>Americas</td>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mok &lt;ipline's1983@icable.ph&gt;</td>\n",
       "      <td>user2.2@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-08-06 07:31:03</td>\n",
       "      <td>Befriend Jenna Jameson</td>\n",
       "      <td>\\nUpgrade your sex and pleasures with these te...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>Americas</td>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daily Top 10 &lt;Karmandeep-opengevl@universalnet...</td>\n",
       "      <td>user2.9@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-08-06 16:28:00</td>\n",
       "      <td>CNN.com Daily Top 10</td>\n",
       "      <td>&gt;+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-12.00</td>\n",
       "      <td>Americas</td>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael Parker &lt;ivqrnai@pobox.com&gt;</td>\n",
       "      <td>SpamAssassin Dev &lt;xrh@spamassassin.apache.org&gt;</td>\n",
       "      <td>2008-08-06 07:31:20</td>\n",
       "      <td>Re: svn commit: r619753 - in /spamassassin/tru...</td>\n",
       "      <td>Would anyone object to removing .so from this ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.00</td>\n",
       "      <td>Americas</td>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gretchen Suggs &lt;externalsep1@loanofficertool.com&gt;</td>\n",
       "      <td>user2.2@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-08-06 07:31:21</td>\n",
       "      <td>SpecialPricesPharmMoreinfo</td>\n",
       "      <td>\\nWelcomeFastShippingCustomerSupport\\nhttp://7...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>Europe/Africa</td>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sender  \\\n",
       "0                   Young Esposito <Young@iworld.de>   \n",
       "1                       Mok <ipline's1983@icable.ph>   \n",
       "2  Daily Top 10 <Karmandeep-opengevl@universalnet...   \n",
       "3                 Michael Parker <ivqrnai@pobox.com>   \n",
       "4  Gretchen Suggs <externalsep1@loanofficertool.com>   \n",
       "\n",
       "                                         receiver                date  \\\n",
       "0                     user4@gvc.ceas-challenge.cc 2008-08-06 07:31:02   \n",
       "1                   user2.2@gvc.ceas-challenge.cc 2008-08-06 07:31:03   \n",
       "2                   user2.9@gvc.ceas-challenge.cc 2008-08-06 16:28:00   \n",
       "3  SpamAssassin Dev <xrh@spamassassin.apache.org> 2008-08-06 07:31:20   \n",
       "4                   user2.2@gvc.ceas-challenge.cc 2008-08-06 07:31:21   \n",
       "\n",
       "                                             subject  \\\n",
       "0                          Never agree to be a loser   \n",
       "1                             Befriend Jenna Jameson   \n",
       "2                               CNN.com Daily Top 10   \n",
       "3  Re: svn commit: r619753 - in /spamassassin/tru...   \n",
       "4                         SpecialPricesPharmMoreinfo   \n",
       "\n",
       "                                                body  label  urls  \\\n",
       "0  Buck up, your troubles caused by small dimensi...      1     1   \n",
       "1  \\nUpgrade your sex and pleasures with these te...      1     1   \n",
       "2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...      1     1   \n",
       "3  Would anyone object to removing .so from this ...      0     1   \n",
       "4  \\nWelcomeFastShippingCustomerSupport\\nhttp://7...      1     1   \n",
       "\n",
       "   timezone_hours timezone_region  year  month  day  hour  day_of_week  \\\n",
       "0           -7.00        Americas  2008      8    6     7            2   \n",
       "1           -5.00        Americas  2008      8    6     7            2   \n",
       "2          -12.00        Americas  2008      8    6    16            2   \n",
       "3           -6.00        Americas  2008      8    6     7            2   \n",
       "4           -4.00   Europe/Africa  2008      8    6     7            2   \n",
       "\n",
       "    day_name  is_weekend  \n",
       "0  Wednesday           0  \n",
       "1  Wednesday           0  \n",
       "2  Wednesday           0  \n",
       "3  Wednesday           0  \n",
       "4  Wednesday           0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA4263_Group_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
